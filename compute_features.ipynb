{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark' in vars():\n",
    "  spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, count, to_timestamp, monotonically_increasing_id, desc, when, sum as _sum, monotonically_increasing_id\n",
    "from pyspark.sql.functions import dayofmonth, weekofyear, month, year\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isnan\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/31 19:35:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Count available cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "# In this case the amount of executors will be equal to the amount of cores\n",
    "instances = cores\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"MoneyLaundering\") \\\n",
    "          .config(\"spark.driver.memory\", \"3g\") \\\n",
    "          .config(\"spark.executor.memory\", \"4g\") \\\n",
    "          .config(\"spark.executor.instances\", cores) \\\n",
    "          .config(\"spark.executor.cores\", cores//instances) \\\n",
    "          .config(\"spark.sql.shuffle.partitions\", cores) \\\n",
    "          .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "          .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataframe = spark.read.parquet(\"src/datasets/my_HI-Small_Trans.parquet\", header=True)\n",
    "dataframe = dataframe.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features of the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureManager import FeatureManager\n",
    "manager = FeatureManager(dataframe)\n",
    "manager.compute_features_of_whole_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundering = manager.dataframe.filter('is_laundering==1')\n",
    "non_laundering = manager.dataframe.filter('is_laundering==0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features of the graph\n",
    "The next step is to understand the structure of the different patterns in order to identify further features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/patterns.png\" style=\"width: 600px\">\n",
    "\n",
    "\n",
    "In order to do that, I thought that the best solution was to process the dataset using GraphFrames, a package for Apache Spark which provides DataFrame-based Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the image below, it would be beneficial to process certain features for each node in the graph to gain valuable insights into the transactions:\n",
    "1. **Compute the number of in-out edges (fan-in, fan-out)** <br>\n",
    "    A transaction involves an exchange between two accounts, and it would be valuable to calculate the connection degrees for each account:\n",
    "    * In-out degrees for the sender account\n",
    "    * In-out degrees for the receiver account\n",
    "    <br><br>\n",
    "2. **Identify intermediary transactions (scatter-gather)** <br>\n",
    "    By analyzing the flow of transactions, we can identify intermediary transactions. These are transactions that act as intermediaries, facilitating the movement of funds between multiple accounts\n",
    "    <br><br>\n",
    "3. **Detect forwarding transactions** <br>\n",
    "    An account receives a sum of money and then forwards it to another account\n",
    "    <br><br>\n",
    "4. **Check for intermediate transactions between two transactions** <br>\n",
    "    We can check if certain transactions act as intermediaries between two other transactions\n",
    "    <br><br>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: green\">Analisi delle transazioni ricorrenti: Identifica transazioni ricorrenti o schemi di transazioni che si verificano frequentemente nel dataset.</p>\n",
    "\n",
    "Analisi di cicli: Identifica cicli nel grafo, poiché potrebbero indicare flussi di denaro chiusi o riciclaggio di fondi.\n",
    "\n",
    "<p style=\"color: yellow\">Distanza tra nodi: Calcola la distanza tra nodi nel grafo, ad esempio la distanza più breve tra due account, per comprendere quanto sono collegati.</p>\n",
    "\n",
    "Centralità dei nodi: Calcola la centralità dei nodi nel grafo, come la centralità di grado, la centralità di intermediazione (betweenness centrality) e la centralità di prossimità. Queste metriche possono aiutare a identificare nodi critici o importanti nel sistema di transazioni.\n",
    "\n",
    "PageRank: calcola i punteggi di PageRank per ciascun account. Il PageRank è una misura dell'importanza di una pagina web nel motore di ricerca Google, ma può essere applicato anche ai grafici delle transazioni per identificare gli account più importanti.\n",
    "\n",
    "Rilevamento comunità: identifica i gruppi di account che sono più strettamente collegati tra loro rispetto al resto del grafico. Questo può essere utile per rilevare reti di frodi o reti di riciclaggio di denaro.\n",
    "\n",
    "Flusso di denaro: Segui il flusso di denaro attraverso i collegamenti del grafo, calcolando i flussi di denaro tra diversi nodi e identificando i percorsi più comuni o significativi.\n",
    "\n",
    "Analisi delle transazioni anomale: Rileva transazioni anomale utilizzando tecniche di rilevamento delle anomalie come il rilevamento degli outlier e l'apprendimento automatico.\n",
    "\n",
    "Analisi temporale: Esamina l'andamento temporale delle transazioni per rilevare trend o cambiamenti nel tempo.\n",
    "\n",
    "\n",
    "Misure di centralità: calcola le misure di centralità come la centralità del grado, la centralità della centralità e la centralità dell'autovettore per ogni account. Queste misure possono aiutare a identificare i conti più importanti nel grafico.\n",
    "\n",
    "Previsione dei collegamenti: utilizza algoritmi di machine learning per prevedere quali account saranno probabilmente connessi in futuro in base alla loro cronologia delle transazioni. Questo può aiutare a identificare potenziali frodi o attività di riciclaggio di denaro prima che si verifichino.\n",
    "\n",
    "Analisi temporale: analizza i modelli temporali delle transazioni per identificare attività insolite, come un improvviso aumento del numero o della quantità di transazioni.\n",
    "\n",
    "<p style=\"color: red\">Analisi delle transazioni internazionali: Identifica transazioni che coinvolgono account di diverse nazioni per rilevare attività internazionale.</p>\n",
    "\n",
    "<p style=\"color: red\">Analisi delle transazioni a valuta: Studia le transazioni che coinvolgono diverse valute per comprendere i flussi di denaro internazionali.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame \n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "class MyGraph:\n",
    "    # create the graph using the vertices and edges found in the dataset taken into account (train or test)\n",
    "    def __init__(self, df):\n",
    "        self.vertices = df.select(\"from_account\")\\\n",
    "                            .withColumnRenamed('from_account', 'id')\\\n",
    "                            .union(df.select(\"to_account\"))\\\n",
    "                            .distinct()\n",
    "\n",
    "        self.edges = df.withColumnRenamed('from_account', 'src')\\\n",
    "            .withColumnRenamed('to_account', 'dst')\n",
    "\n",
    "        self.g = GraphFrame(self.vertices, self.edges)\n",
    "        self.df = df\n",
    "        self.compute_inOut_degrees()\n",
    "\n",
    "    def compute_inOut_degrees(self):\n",
    "        vertexInDegrees = self.g.inDegrees\n",
    "        vertexOutDegrees = self.g.outDegrees\n",
    "        vertices = vertexInDegrees.join(vertexOutDegrees, 'id', 'fullouter').fillna(0)\n",
    "\n",
    "        dataset = self.df.alias('df').join(vertices.alias('vertices'), [self.df.from_account==vertices.id], 'left')\\\n",
    "            .select('df.*', \n",
    "                    col('vertices.inDegree').alias('from_account_inDegree'),\n",
    "                    col('vertices.outDegree').alias('from_account_outDegree'))\\\n",
    "                        .alias('df_from')\n",
    "\n",
    "        dataset = dataset.join(vertices.alias('vertices'), [self.df.to_account==vertices.id], 'left')\\\n",
    "                .select('df_from.*', \n",
    "                    col('vertices.inDegree').alias('to_account_inDegree'),\n",
    "                    col('vertices.outDegree').alias('to_account_outDegree'))\n",
    "\n",
    "        self.df = dataset\n",
    "\n",
    "    def get_forwards(self):\n",
    "        # it consists in getting all transactions in which the receiver of the transaction \n",
    "        # sends the same amount of received money to another account\n",
    "        # OUTPUT: id of inolved transactions where:\n",
    "        # - before_forward will be setted to 1 if a transaction is that one before a secondly forwarding transaction\n",
    "        # - forward will be setted to 1 if a transaction is that one that makes the forward\n",
    "\n",
    "        motif = \"(a)-[e]->(b); (b)-[e2]->(c)\"\n",
    "        forwards = self.g.find(motif).filter(\"e.amount_received == e2.amount_paid and e.timestamp <= e2.timestamp and a!=b and b!=c\")\n",
    "        forwards = forwards#.select(col('e.id').alias('before_forward'),col('e2.id').alias('forward'))\n",
    "\n",
    "        before_forward = forwards.select(col('e.id').alias('id'))\\\n",
    "            .distinct()\\\n",
    "            .withColumn('before_forward',lit(1))\n",
    "        # distinct: I can use it, or I can count how many times the id is involved\n",
    "        forward = forwards.select(col('e2.id').alias('id'))\\\n",
    "            .distinct()\\\n",
    "            .withColumn('forward',lit(1))\n",
    "        # distinct: I can use it, or I can count how many times the id is involved\n",
    "    \n",
    "        self.df = self.df.join(before_forward, 'id','left').join(forward, 'id','left').na.fill(value=0,subset=['before_forward','forward'])\n",
    "\n",
    "    def get_distance(self):\n",
    "        # get the distance between two nodes \n",
    "        return \n",
    "    \n",
    "    def same_or_similar(self):\n",
    "        # it search if for each transaction there is:\n",
    "        # - another transaction with the same attributes, except the amounts \n",
    "        # - another transaction with similar attributes, except the timestamps and amounts\n",
    "        motif = \"(a)-[t1]->(b); (a)-[t2]->(b)\"\n",
    "\n",
    "        same_where = 't1.timestamp == t2.timestamp and \\\n",
    "                        t1.from_bank == t2.from_bank and \\\n",
    "                        t1.to_bank == t2.to_bank and \\\n",
    "                        t1.payment_currency == t2.payment_currency and \\\n",
    "                        t1.receiving_currency == t2.receiving_currency and \\\n",
    "                        t1.payment_format == t2.payment_format and \\\n",
    "                        t1.amount_paid != t2.amount_paid and \\\n",
    "                        t1.id != t2.id'\n",
    "        \n",
    "        same = self.g.find(motif).filter(same_where).select('t1.id').withColumn('exist_same',lit(1)).distinct()\n",
    "\n",
    "        similar_where = 't1.timestamp != t2.timestamp and \\\n",
    "                        t1.from_bank == t2.from_bank and \\\n",
    "                        t1.to_bank == t2.to_bank and \\\n",
    "                        t1.payment_currency == t2.payment_currency and \\\n",
    "                        t1.receiving_currency == t2.receiving_currency and \\\n",
    "                        t1.payment_format == t2.payment_format and \\\n",
    "                        t1.amount_paid != t2.amount_paid'\n",
    "\n",
    "        \n",
    "        similar = self.g.find(motif).filter(similar_where).select('t1.id').withColumn('exist_similar',lit(1)).distinct()\n",
    "        \n",
    "        self.df = self.df.join(similar, 'id', 'left').join(same, 'id', 'left').na.fill(value=0,subset=['exist_same','exist_similar'])\n",
    "\n",
    "    def build_temp_ds(self):\n",
    "        return\n",
    "    \n",
    "    def compute_fan_in(self):\n",
    "        # as explained in undestand_pattern.ipynb it is useful to compute the following feature: \n",
    "        # - for each to_account, the number of incoming nodes to the same bank and all in node must have the same: \n",
    "        #     * receiving_currency \n",
    "        #     * payment_currency\n",
    "        #     * payment_format\n",
    "        #     * there must be at most 4 days between the first transaction and the last in the series\n",
    "\n",
    "        motif = \"(a)-[t1]->(b); (c)-[t2]->(b)\"\n",
    "        \n",
    "        fan_in_query = 'abs(datediff(t1.timestamp, t2.timestamp)) <= 4 and \\\n",
    "                    t1.to_bank == t2.to_bank and \\\n",
    "                    t1.payment_currency == t2.payment_currency and \\\n",
    "                    t1.receiving_currency == t2.receiving_currency and \\\n",
    "                    t1.payment_format == t2.payment_format'\n",
    "                \n",
    "        fan_in = self.g.find(motif).filter(fan_in_query).select('a', 'b', 't1')\n",
    "        fan_in = fan_in.groupBy('a', 'b', 't1').count().select('t1.id',col('count').alias('fan_in_degree'))\n",
    "\n",
    "        self.df = self.df.join(fan_in, 'id','left')\n",
    "\n",
    "    def compute_fan_out(self):\n",
    "        # as explained in undestand_pattern.ipynb it is useful to compute the following feature: \n",
    "        # - for each from_account, the number of outgoing nodes to the same bank and all in node must have the same: \n",
    "        #     * payment_format\n",
    "        #     * there must be at most 4 days between the first transaction and the last in the series\n",
    "        vertices = self.df.select(\"from_account\")\\\n",
    "                        .withColumnRenamed('from_account', 'id')\\\n",
    "                        .union(self.df.select(\"to_account\"))\\\n",
    "                        .distinct()\n",
    "\n",
    "        edges = self.df.withColumnRenamed('from_account', 'src')\\\n",
    "            .withColumnRenamed('to_account', 'dst').filter('src!=dst and payment_currency==receiving_currency and payment_format==\"ACH\"')\\\n",
    "            .select('id','timestamp','src','dst','payment_currency','payment_format','from_bank')\n",
    "        \n",
    "        g = GraphFrame(vertices, edges)\n",
    "\n",
    "        motif = \"(a)-[t1]->(b); (a)-[t2]->(c)\"\n",
    "        \n",
    "        fan_out_query = 'abs(datediff(t1.timestamp, t2.timestamp)) <= 4 and \\\n",
    "                        t1.from_bank == t2.from_bank and \\\n",
    "                        a != b and a != c and b != c and\\\n",
    "                        t1.id != t2.id'\n",
    "                \n",
    "        fan_out = g.find(motif).filter(fan_out_query).select('a', 'b', 'c', 't1.id')\n",
    "        fan_out = fan_out.groupBy('a','b','c','id').count()\n",
    "        fan_out = fan_out.groupBy('id').agg(count('*').alias('fan_out_degree')).select('id', 'fan_out_degree').withColumn('fan_out_degree', col('fan_out_degree')+1)\n",
    "        \n",
    "        self.df = self.df.join(fan_out, 'id', 'left').na.fill(value=1,subset=['fan_out_degree'])\n",
    "\n",
    "    def compute_fan(self):\n",
    "        self.compute_fan_in()\n",
    "        self.compute_fan_out()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonemalesardi/spark/python/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/Users/simonemalesardi/spark/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "my_graph = MyGraph(dataframe)\n",
    "my_graph.compute_fan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph.df.sortBy(col(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection_degree(df):\n",
    "    connection_degree = df.groupBy('from_account','to_account').count()\n",
    "\n",
    "def similar_transaction(df):\n",
    "    return \n",
    "\n",
    "def get_fan(df):\n",
    "    # count transaction:\n",
    "    # - made by account in a bank with a currency --> outgoings('from_account','from_bank','payment_currency') -> fan-out\n",
    "    # - recieved by account in a bank with a currency --> ingoings('to_account','to_bank','receiving_currency') -> fan-in\n",
    "    outgoings = df.groupBy('from_account','from_bank','payment_currency').count()\n",
    "    ingoings = df.groupBy('to_account','to_bank','receiving_currency').count()\n",
    "\n",
    "    df = df.alias('orig').join(outgoings.alias('outgoings'), \n",
    "                          [df.from_account==outgoings.from_account,\n",
    "                           df.from_bank==outgoings.from_bank,\n",
    "                           df.payment_currency==outgoings.payment_currency], how=\"left\")\\\n",
    "                            .select('orig.*',col('outgoings.count').alias('outgoings'))\n",
    "    \n",
    "    df = df.alias('outgoings').join(ingoings.alias('ingoings'), \n",
    "                          [df.to_account==ingoings.to_account,\n",
    "                           df.to_bank==ingoings.to_bank,\n",
    "                           df.receiving_currency==ingoings.receiving_currency], how=\"left\")\\\n",
    "                            .select('outgoings.*',col('ingoings.count').alias('ingoings'))\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph.compute_inOut_degrees()\n",
    "my_graph.df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph.same_or_similar()\n",
    "new_df = get_fan(my_graph.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the example below is an example of \"similar\" transaction\n",
    "# dataframe.filter('from_bank==10 and from_account==\"800043990\" and to_bank==10 and to_account==\"800043990\" and receiving_currency==\"US Dollar\"').show()\n",
    "# the example below is an example of \"same\" transaction\n",
    "# dataframe.filter('timestamp==\"2022-09-01 00:07:00\" and from_bank==1601 and from_account==\"8005D0700\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph.get_forwards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph.df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fan-in and Fan-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.filter('id==17478').show()\n",
    "dataframe.filter('from_account==\"800737690\" and to_account!=\"800737690\" and abs(datediff(\"2022-09-01 04:33:00\",timestamp)) <= 4 and payment_format==\"ACH\" and to_account!=\"80020C5B0\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.filter('to_account==\"812BD4500\" and abs(datediff(\"2022-09-01 00:45:00\",timestamp))<=4 and\\\n",
    "                  receiving_currency==\"Euro\" and payment_format==\"Cheque\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.filter('to_account==\"8001275E0\" and abs(datediff(\"2022-09-05 11:00:00\",timestamp))<=4 and\\\n",
    "                  receiving_currency==\"US Dollar\" and payment_format==\"Cheque\"').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_sm2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
