{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/03 08:08:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "\n",
    "if 'spark' in vars():\n",
    "  spark.stop()\n",
    "\n",
    "# Count available cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "# In this case the amount of executors will be equal to the amount of cores\n",
    "instances = cores\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"MoneyLaundering\") \\\n",
    "          .config(\"spark.driver.memory\", \"3g\") \\\n",
    "          .config(\"spark.executor.memory\", \"4g\") \\\n",
    "          .config(\"spark.executor.instances\", cores) \\\n",
    "          .config(\"spark.executor.cores\", cores//instances) \\\n",
    "          .config(\"spark.sql.shuffle.partitions\", cores) \\\n",
    "          .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "          .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "dataframe = spark.read.parquet(\"src/datasets/my_HI-Small_Trans.parquet\", header=True)\n",
    "dataframe = dataframe.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features of the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "manager = FeatureManager(dataframe)\n",
    "manager.compute_features_of_whole_df()\n",
    "# laundering = manager.dataframe.filter('is_laundering==1')\n",
    "# non_laundering = manager.dataframe.filter('is_laundering==0')\n",
    "\n",
    "ach_mapping = manager.ach_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'timestamp',\n",
       " 'from_account',\n",
       " 'to_account',\n",
       " 'same_account',\n",
       " 'from_bank',\n",
       " 'to_bank',\n",
       " 'same_bank',\n",
       " 'amount_received',\n",
       " 'amount_paid',\n",
       " 'same_amounts',\n",
       " 'receiving_currency',\n",
       " 'payment_currency',\n",
       " 'same_currency',\n",
       " 'payment_format',\n",
       " 'is_laundering']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro: Compute features of the graph\n",
    "The next step is to understand the structure of the different patterns in order to identify further features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/patterns.png\" style=\"width: 600px\">\n",
    "\n",
    "\n",
    "In order to do that, I thought that the best solution was to process the dataset using GraphFrames, a package for Apache Spark which provides DataFrame-based Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the image below, it would be beneficial to process certain features for each node in the graph to gain valuable insights into the transactions:\n",
    "1. **Compute the number of in-out edges (fan-in, fan-out)** <br>\n",
    "    A transaction involves an exchange between two accounts, and it would be valuable to calculate the connection degrees for each account:\n",
    "    * In-out degrees for the sender account\n",
    "    * In-out degrees for the receiver account\n",
    "    <br><br>\n",
    "2. **Identify intermediary transactions (scatter-gather)** <br>\n",
    "    By analyzing the flow of transactions, we can identify intermediary transactions. These are transactions that act as intermediaries, facilitating the movement of funds between multiple accounts\n",
    "    <br><br>\n",
    "3. **Detect forwarding transactions** <br>\n",
    "    An account receives a sum of money and then forwards it to another account\n",
    "    <br><br>\n",
    "4. **Check for intermediate transactions between two transactions** <br>\n",
    "    We can check if certain transactions act as intermediaries between two other transactions\n",
    "    <br><br>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonemalesardi/spark/python/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/Users/simonemalesardi/spark/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding cycles of degree 2...\n",
      "adding cycles of degree 3...\n",
      "adding cycles of degree 4...\n",
      "adding cycles of degree 5...\n",
      "adding cycles of degree 6...\n",
      "adding cycles of degree 7...\n",
      "adding cycles of degree 8...\n",
      "adding cycles of degree 9...\n"
     ]
    }
   ],
   "source": [
    "my_graph = MyGraph(dataframe)\n",
    "my_graph.get_forwards()\n",
    "my_graph.same_or_similar()\n",
    "my_graph.compute_fan()\n",
    "my_graph.find_cycles(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph.join_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding cycles of degree 2...\n",
      "adding cycles of degree 3...\n",
      "adding cycles of degree 4...\n",
      "adding cycles of degree 5...\n",
      "adding cycles of degree 6...\n",
      "adding cycles of degree 7...\n",
      "adding cycles of degree 8...\n",
      "adding cycles of degree 9...\n",
      "adding cycles of degree 10...\n",
      "adding cycles of degree 11...\n",
      "adding cycles of degree 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4317:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+--------+\n",
      "|   id|min_cycle|max_cycle|involved|\n",
      "+-----+---------+---------+--------+\n",
      "|22937|        2|        2|       1|\n",
      "|33710|        2|        2|       1|\n",
      "|40064|        2|        4|       1|\n",
      "|40065|        2|        4|       1|\n",
      "|41892|        2|        2|       1|\n",
      "|41955|        2|        2|       1|\n",
      "|42963|        2|        2|       1|\n",
      "|45546|        2|        2|       1|\n",
      "|46498|        2|        3|       1|\n",
      "|47454|     null|     null|       1|\n",
      "|47455|     null|     null|       1|\n",
      "|47456|     null|     null|       1|\n",
      "|48067|        2|        2|       1|\n",
      "|49417|        2|        2|       1|\n",
      "|49430|        2|        2|       1|\n",
      "|51703|        2|        2|       1|\n",
      "|51863|        2|        2|       1|\n",
      "|51874|        2|        2|       1|\n",
      "|53161|        2|        2|       1|\n",
      "|53650|        2|        2|       1|\n",
      "+-----+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_graph = MyGraph(dataframe)\n",
    "my_graph.find_cycles(12)\n",
    "my_graph.cycles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+--------+\n",
      "|         id|min_cycle|max_cycle|involved|\n",
      "+-----------+---------+---------+--------+\n",
      "| 8590127893|        6|        6|       1|\n",
      "|     399300|        7|        7|       1|\n",
      "|17180380682|        8|        8|       1|\n",
      "|17180336093|        8|        8|       1|\n",
      "|51540230598|        8|        8|       1|\n",
      "|42950030184|        8|        8|       1|\n",
      "|60129581425|       10|       10|       1|\n",
      "|34360334979|       10|       10|       1|\n",
      "|51539950677|       10|       10|       1|\n",
      "|34359785209|       10|       10|       1|\n",
      "|42949745997|       10|       10|       1|\n",
      "+-----------+---------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_graph.cycles.filter('min_cycle>5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the example below is an example of \"similar\" transaction\n",
    "# dataframe.filter('from_bank==10 and from_account==\"800043990\" and to_bank==10 and to_account==\"800043990\" and receiving_currency==\"US Dollar\"').show()\n",
    "# the example below is an example of \"same\" transaction\n",
    "# dataframe.filter('timestamp==\"2022-09-01 00:07:00\" and from_bank==1601 and from_account==\"8005D0700\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fan-in and Fan-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.filter('id==17478').show()\n",
    "dataframe.filter('from_account==\"800737690\" and to_account!=\"800737690\" and abs(datediff(\"2022-09-01 04:33:00\",timestamp)) <= 4 and payment_format==\"ACH\" and to_account!=\"80020C5B0\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.filter('to_account==\"812BD4500\" and abs(datediff(\"2022-09-01 00:45:00\",timestamp))<=4 and\\\n",
    "                  receiving_currency==\"Euro\" and payment_format==\"Cheque\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.filter('to_account==\"8001275E0\" and abs(datediff(\"2022-09-05 11:00:00\",timestamp))<=4 and\\\n",
    "                  receiving_currency==\"US Dollar\" and payment_format==\"Cheque\"').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_sm2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
