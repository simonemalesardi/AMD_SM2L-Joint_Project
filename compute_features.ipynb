{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from math import isnan\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graphframes import *\n",
    "\n",
    "########## START - PYSPARK ##########\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr, count, to_timestamp, monotonically_increasing_id, \\\n",
    "    desc, sum as _sum, min, max, rand, when, \\\n",
    "    datediff, dayofmonth, weekofyear, month, year, hour, dayofweek, \\\n",
    "    unix_timestamp, array, lit, round\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer \n",
    "########## END - PYSPARK ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/18 14:57:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count available cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "# In this case the amount of executors will be equal to the amount of cores\n",
    "instances = cores\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"AMD-SM2L Joint Project\") \\\n",
    "          .config(\"spark.driver.memory\", \"3g\") \\\n",
    "          .config(\"spark.executor.memory\", \"4g\") \\\n",
    "          .config(\"spark.executor.instances\", cores) \\\n",
    "          .config(\"spark.executor.cores\", cores//instances) \\\n",
    "          .config(\"spark.sql.shuffle.partitions\", cores) \\\n",
    "          .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "          .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "dataframe = spark.read.parquet(\"src/datasets/my_HI-Small_Trans.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureManager:\n",
    "    def __init__(self, dataframe):\n",
    "        # Percorso del file Parquet\n",
    "        self.origDF = dataframe\n",
    "        \n",
    "    def compute_features_of_whole_df(self):\n",
    "        self.df_features = self.origDF\\\n",
    "            .withColumnRenamed('receiving_currency', 'rec_cur')\\\n",
    "            .withColumnRenamed('payment_currency', 'pay_cur')\\\n",
    "            .withColumnRenamed('payment_format', 'pay_for')\\\n",
    "\n",
    "        currencies = self.df_features.select('rec_cur').distinct().union(self.df_features.select('pay_cur').distinct())\n",
    "        \n",
    "        currency = StringIndexer(inputCol='rec_cur', outputCol='receiving_currency')\n",
    "        payment_format = StringIndexer(inputCol='pay_for', outputCol='payment_format')\n",
    "        \n",
    "        rec_currency_model = currency.fit(currencies)\n",
    "        self.df_features = rec_currency_model.transform(self.df_features)\n",
    "        pay_currency_model = rec_currency_model.setInputCol('pay_cur').setOutputCol('payment_currency')\n",
    "        self.df_features = pay_currency_model.transform(self.df_features)\n",
    "        \n",
    "        payment_format_model = payment_format.fit(self.df_features)\n",
    "        self.df_features = payment_format_model.transform(self.df_features)\n",
    "        self.ach_mapping = {v: k for k, v in dict(enumerate(payment_format_model.labels)).items()}\n",
    "        \n",
    "        column_order = ['id', 'timestamp',\n",
    "                        'from_account','to_account','same_account',\n",
    "                        'from_bank','to_bank','same_bank',\n",
    "                        'amount_received','amount_paid','amount_difference','same_amounts',\n",
    "                        'receiving_currency','payment_currency','same_currency',\n",
    "                        'payment_format', 'is_laundering']\n",
    "        \n",
    "        self.df_features = self.df_features\\\n",
    "                .withColumn('same_bank', (col('from_bank')==col('to_bank')).cast('integer'))\\\n",
    "                .withColumn('same_account', (col('from_account')==col('to_account')).cast('integer'))\\\n",
    "                .withColumn('same_currency', (col('receiving_currency')==col('payment_currency')).cast('integer'))\\\n",
    "                .withColumn('same_amounts', (col('amount_received')==col('amount_paid')).cast('integer'))\\\n",
    "                .withColumn('amount_difference', (col('amount_paid')-col('amount_received')))\\\n",
    "                .select(column_order)\n",
    "\n",
    "        #self.df_features.write.parquet('src/datasets/HI-Small_features.parquet')\n",
    "\n",
    "    def compute_timestamp_features(self, train=True):\n",
    "        df = self.train_df if train else self.test_df\n",
    "\n",
    "        df = df\\\n",
    "            .withColumn('week', weekofyear(\"timestamp\"))\\\n",
    "            .withColumn('day_of_month', dayofmonth(\"timestamp\"))\\\n",
    "            .withColumn('day_of_week', dayofweek(\"timestamp\"))\\\n",
    "            .withColumn('hour', hour(\"timestamp\"))\n",
    "\n",
    "        hour_FA = df.groupBy('from_account','hour').agg(count('*').alias('transactions_same_hour_fa'))\n",
    "        day_of_month_FA = df.groupBy('from_account','day_of_month').agg(count('*').alias('transactions_same_day_fa'))\n",
    "        week_FA = df.groupBy('from_account','week').agg(count('*').alias('transactions_same_week_fa'))\n",
    "        \n",
    "        hour_FA_TA = df.groupBy('from_account','to_account', 'hour').agg(count('*').alias('transactions_same_hour_fata'))\n",
    "        day_of_month_FA_TA = df.groupBy('from_account','to_account','day_of_month').agg(count('*').alias('transactions_same_day_fata'))\n",
    "        week_FA_TA = df.groupBy('from_account','to_account','week').agg(count('*').alias('transactions_same_week_fata'))\n",
    "        \n",
    "        df = df\\\n",
    "            .join(hour_FA, ['from_account', 'hour'], 'left')\\\n",
    "            .join(day_of_month_FA, ['from_account', 'day_of_month'], 'left')\\\n",
    "            .join(week_FA, ['from_account', 'week'], 'left')\\\n",
    "            .join(hour_FA_TA, ['from_account', 'to_account', 'hour'], 'left')\\\n",
    "            .join(day_of_month_FA_TA, ['from_account', 'to_account', 'day_of_month'], 'left')\\\n",
    "            .join(week_FA_TA, ['from_account', 'to_account', 'week'], 'left')\n",
    "        \n",
    "        if train: \n",
    "            self.train_df = df\n",
    "        else: \n",
    "            self.test_df = df\n",
    "\n",
    "    def dataset_sampling(self):\n",
    "        # ha senso campionare il dataset prendendo quei giorni in cui le transazioni non hanno uno sbilanciamento di classe troppo elevato\n",
    "        # posso prendere dal giorno 11 in poi e anche un paio di giorni del \n",
    "        return \n",
    "\n",
    "    def drop_columns(self):\n",
    "        cols = ('rec_cur','pay_cur','pay_for','from_bank','to_bank','from_account','to_account')\n",
    "        self.dataframe = self.dataframe.drop(*cols)\n",
    "\n",
    "    def split_original_datasets(self):\n",
    "        # classes are unbalanced, so we need to take a similar number of laundering and non laundering transactions:\n",
    "        #Â in order to do that the dataset is filtered taking randomically a limited number of non laundering transactions \n",
    "        # and then laundering transactions are added \n",
    "        self.origDF = self.origDF.drop('from_bank','to_bank')\n",
    "        launderings = self.origDF.filter('is_laundering==1')\n",
    "\n",
    "        print('\\nSplitting dataframe into train and test set...')\n",
    "        sample_non_laundering = self.origDF.filter('is_laundering==0').orderBy(rand()).limit(launderings.count())\n",
    "        sample_dataset = sample_non_laundering.union(launderings.filter('is_laundering==1'))\n",
    "\n",
    "        self.train_df, test_df = sample_dataset.randomSplit([0.8, 0.2])\n",
    "\n",
    "        subtracted = self.origDF.subtract(sample_dataset)\n",
    "        self.test_df = test_df.union(subtracted)   \n",
    "        print('Dataframe splitted')\n",
    "\n",
    "    def save_dataframes(self, path):\n",
    "        self.train_df.write.parquet(path.format('train_set'))\n",
    "        self.test_df.write.parquet(path.format('test_set'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features of the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "manager = FeatureManager(dataframe)\n",
    "manager.compute_features_of_whole_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting dataframe into train and test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe splitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "manager.split_original_datasets()\n",
    "manager.compute_timestamp_features(True)\n",
    "manager.compute_timestamp_features(False)\n",
    "manager.save_dataframes('src/datasets/HI_Small_{}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features of the graph\n",
    "The next step is to understand the structure of the different patterns in order to identify further features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/patterns.png\" style=\"width: 600px\">\n",
    "\n",
    "\n",
    "In order to do that, I thought that the best solution was to process the dataset using GraphFrames, a package for Apache Spark which provides DataFrame-based Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the image below, it would be beneficial to process certain features for each node in the graph to gain valuable insights into the transactions:\n",
    "1. **Compute the number of in-out edges (fan-in, fan-out)** <br>\n",
    "    A transaction involves an exchange between two accounts, and it would be valuable to calculate the connection degrees for each account:\n",
    "    * In-out degrees for the sender account\n",
    "    * In-out degrees for the receiver account\n",
    "    <br><br>\n",
    "2. **Identify intermediary transactions (scatter-gather)** <br>\n",
    "    By analyzing the flow of transactions, we can identify intermediary transactions. These are transactions that act as intermediaries, facilitating the movement of funds between multiple accounts\n",
    "    <br><br>\n",
    "3. **Detect forwarding transactions** <br>\n",
    "    An account receives a sum of money and then forwards it to another account\n",
    "    <br><br>\n",
    "4. **Check for intermediate transactions between two transactions** <br>\n",
    "    We can check if certain transactions act as intermediaries between two other transactions\n",
    "    <br><br>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGraph:\n",
    "    # create the graph using the vertices and edges found in the dataset taken into account (train or test)\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.ids = self.df.select('id','from_account','to_account')\n",
    "        self.vertices, self.edges, self.g = self.create_graph()\n",
    "        self.compute_inOut_degrees()\n",
    "\n",
    "    def create_graph(self, init=True):\n",
    "        vertices = self.df.select(\"from_account\")\\\n",
    "                            .withColumnRenamed('from_account', 'id')\\\n",
    "                            .union(self.ids.select(\"to_account\"))\\\n",
    "                            .distinct()\n",
    "        if init:\n",
    "            edges = self.df.withColumnRenamed('from_account', 'src')\\\n",
    "                .withColumnRenamed('to_account', 'dst')\n",
    "        else:\n",
    "            edges = self.df.withColumnRenamed('from_account', 'src')\\\n",
    "                .withColumnRenamed('to_account', 'dst').filter('from_account!=to_account and receiving_currency==payment_currency and payment_format=\"ACH\"')\\\n",
    "                .select('id','timestamp','src','dst','payment_currency','payment_format')\n",
    "\n",
    "        g = GraphFrame(vertices, edges)\n",
    "        return vertices, edges, g\n",
    "\n",
    "    def compute_inOut_degrees(self):\n",
    "        # for each account, it computes the number of ingoing and outgoing transactions \n",
    "        vertexInDegrees = self.g.inDegrees\n",
    "        vertexOutDegrees = self.g.outDegrees\n",
    "        vertices = vertexInDegrees.join(vertexOutDegrees, 'id', 'fullouter').fillna(0)\n",
    "        \n",
    "        vertices = vertices.withColumnRenamed('id', 'from_account')\n",
    "        self.ids = self.ids.alias('df').join(vertices.alias('vertices'), 'from_account', 'left')\\\n",
    "                        .withColumnRenamed('inDegree','from_account_inDegree')\\\n",
    "                        .withColumnRenamed('outDegree','from_account_outDegree')\n",
    "\n",
    "        vertices = vertices.withColumnRenamed('from_account', 'to_account')\n",
    "        self.ids = self.ids.join(vertices.alias('vertices'), 'to_account', 'left')\\\n",
    "                    .withColumnRenamed('inDegree','to_account_inDegree')\\\n",
    "                    .withColumnRenamed('outDegree','to_account_outDegree')\n",
    "\n",
    "    def get_forwards(self):\n",
    "        # it consists in getting all transactions in which the receiver of the transaction \n",
    "        # sends the same amount of received money to another account\n",
    "        # OUTPUT: id of inolved transactions where:\n",
    "        # - before_forward: 1 if a transaction is that one before a secondly forwarding transaction\n",
    "        # - forward: 1 if a transaction is that one that makes the forward\n",
    "\n",
    "        motif = \"(a)-[e]->(b); (b)-[e2]->(c)\"\n",
    "        forwards = self.g.find(motif).filter(\"e.amount_received == e2.amount_paid and e.timestamp <= e2.timestamp and a!=b and b!=c\")\n",
    "    \n",
    "        before_forward = forwards.select(col('e.id').alias('id'))\\\n",
    "            .distinct()\\\n",
    "            .withColumn('before_forward',lit(1))\n",
    "        # distinct: I can use it, or I can count how many times the id is involved\n",
    "        forward = forwards.select(col('e2.id').alias('id'))\\\n",
    "            .distinct()\\\n",
    "            .withColumn('forward',lit(1))\n",
    "        # distinct: I can use it, or I can count how many times the id is involved\n",
    "    \n",
    "        self.forwards = before_forward.join(forward, 'id','left')#.na.fill(value=0,subset=['before_forward','forward'])\n",
    "     \n",
    "    def same_or_similar(self):\n",
    "        # it search if for each transaction there is:\n",
    "        # - another transaction with the same attributes, except the amounts (exists_same)\n",
    "        # - another transaction with similar attributes, except the timestamps and amounts (exists_similar)\n",
    "        motif = \"(a)-[t1]->(b); (a)-[t2]->(b)\"\n",
    "\n",
    "        same_where = 't1.timestamp == t2.timestamp and \\\n",
    "                        t1.payment_currency == t2.payment_currency and \\\n",
    "                        t1.receiving_currency == t2.receiving_currency and \\\n",
    "                        t1.payment_format == t2.payment_format and \\\n",
    "                        t1.amount_paid != t2.amount_paid and \\\n",
    "                        t1.id != t2.id'\n",
    "        \n",
    "        self.same = self.g.find(motif).filter(same_where).select('t1.id').withColumn('exists_same',lit(1)).distinct()\n",
    "\n",
    "        similar_where = 't1.timestamp != t2.timestamp and \\\n",
    "                        t1.payment_currency == t2.payment_currency and \\\n",
    "                        t1.receiving_currency == t2.receiving_currency and \\\n",
    "                        t1.payment_format == t2.payment_format and \\\n",
    "                        t1.amount_paid != t2.amount_paid'\n",
    "\n",
    "        \n",
    "        self.similar = self.g.find(motif).filter(similar_where).select('t1.id').withColumn('exists_similar',lit(1)).distinct()\n",
    "########## START - FAN PATTERN ##########\n",
    "    def compute_fan_in(self):\n",
    "        \"\"\"\n",
    "            as explained in undestand_pattern.ipynb it is useful to compute the following feature: \n",
    "            - for each to_account, the number of incoming nodes to the same bank and all in node must have the same: \n",
    "                * receiving_currency \n",
    "                * payment_currency\n",
    "                * payment_format\n",
    "                * there must be at most 4 days between the first transaction and the last in the series\n",
    "        \"\"\"\n",
    "        motif = \"(a)-[t1]->(b); (c)-[t2]->(b)\"\n",
    "        \n",
    "        fan_in_query = 'abs(datediff(t1.timestamp, t2.timestamp)) <= 4 and \\\n",
    "                    t1.payment_currency == t2.payment_currency and \\\n",
    "                    t1.receiving_currency == t2.receiving_currency and \\\n",
    "                    t1.payment_format == t2.payment_format'\n",
    "                \n",
    "        fan_in = self.g.find(motif).filter(fan_in_query).select('a', 'b', 't1')\n",
    "        fan_in = fan_in.groupBy('a', 'b', 't1').count().select('t1.id',col('count').alias('fan_in_degree'))\n",
    "\n",
    "        return fan_in\n",
    "\n",
    "    def compute_fan_out(self):\n",
    "        \"\"\"\n",
    "            as explained in undestand_pattern.ipynb it is useful to compute the following feature: \n",
    "            - for each from_account, the number of outgoing nodes to the same bank and all in node must have the same: \n",
    "                * payment_format\n",
    "                * there must be at most 4 days between the first transaction and the last in the series\n",
    "            \n",
    "            in order to handle the big amount of data, data are firstly filtered:\n",
    "            - self transaction (from_account == to_account) doesn't exist in the same fan-out\n",
    "            - two similar transactions (t1(from_account, to_account) == t2(from_account, to_account) ) don't exist in the same fan-out \n",
    "            - fan-outs have ACH payment_format\n",
    "        \"\"\"\n",
    "        _, _, g = self.create_graph(False)\n",
    "\n",
    "        motif = \"(a)-[t1]->(b); (a)-[t2]->(c)\"\n",
    "        \n",
    "        fan_out_query = 'abs(datediff(t1.timestamp, t2.timestamp)) <= 4 and \\\n",
    "                        a != b and a != c and b != c and\\\n",
    "                        t1.id != t2.id'\n",
    "                \n",
    "        fan_out = g.find(motif).filter(fan_out_query).select('a', 'b', 'c', 't1.id')\n",
    "        fan_out = fan_out.groupBy('a','b','c','id').count()\n",
    "        fan_out = fan_out.groupBy('id').agg(count('*').alias('fan_out_degree')).select('id', 'fan_out_degree').withColumn('fan_out_degree', col('fan_out_degree')+1)\n",
    "        \n",
    "        return fan_out\n",
    "    \n",
    "    def compute_fan(self):\n",
    "        fan_in = self.compute_fan_in()\n",
    "        fan_out = self.compute_fan_out()  \n",
    "        \n",
    "        self.fans = fan_in.join(fan_out, 'id', 'fullouter')\n",
    "########## END - FAN PATTERN ##########\n",
    "\n",
    "########## START - CYCLE PATTERN ##########\n",
    "    def generate_combinations(self,accounts):\n",
    "        accounts = set(accounts)\n",
    "        conditions = []\n",
    "        for acc in accounts: \n",
    "            for acc2 in accounts:\n",
    "                if acc!=acc2:\n",
    "                    if ((acc, acc2) not in conditions) and ((acc2, acc) not in conditions):\n",
    "                        conditions.append((acc,acc2))\n",
    "\n",
    "        return ' and '.join(conditions[k][0]+'!='+conditions[k][1] for k in range(len(conditions)))\n",
    "\n",
    "    def build_rules_of_cycles(self, max_iter):\n",
    "        alphabet = list(map(chr, range(97, 123)))\n",
    "        start = 2\n",
    "\n",
    "        rules = []\n",
    "        \n",
    "        for i in range(start-1, max_iter+1):\n",
    "            full_rule = []\n",
    "            single_query = []\n",
    "            transactions = []\n",
    "            receiving_accounts = []\n",
    "            select = []\n",
    "            accounts = []\n",
    "            for j in range(0, i+1):\n",
    "                receiving_account = alphabet[j+1] if j < i else alphabet[0]\n",
    "                receiving_accounts.append(receiving_account)\n",
    "\n",
    "                single_transaction = 't{}'.format(j+1)\n",
    "                transactions.append(single_transaction)\n",
    "\n",
    "                starting_account = alphabet[j]\n",
    "\n",
    "                single_rule = \"({})-[{}]->({})\".format(starting_account, single_transaction, receiving_account)\n",
    "                accounts.append(starting_account)\n",
    "                accounts.append(receiving_account)\n",
    "\n",
    "                full_rule.append(single_rule)    \n",
    "                select.append(single_transaction)\n",
    "            \n",
    "            single_query.append(' and '.join(transactions[k] + '.timestamp <= ' + transactions[k+1] + '.timestamp' for k in range(len(transactions) - 1)))\n",
    "            single_query.append(self.generate_combinations(accounts))\n",
    "        \n",
    "            rules.append(('; '.join(full_rule), ' and '.join(single_query), select, (i+1)))\n",
    "\n",
    "        return rules\n",
    "\n",
    "    def find_cycles(self, max_iter):\n",
    "        # this method obtains 3 features: \n",
    "        # - min_cycle: != 0 if the transaction is the starting one of a cycle\n",
    "        # - max_cycle: != 0 if the transaction is the starting one of a cycle (== min_cycle if there's only that kind of degree)\n",
    "        #Â - involved: 1 if the transaction is involved in a cycle, 0 otherwise\n",
    "        _, _, g = self.create_graph(False)\n",
    "        \n",
    "        created_df = False\n",
    "\n",
    "        max_iter = 1 if (max_iter-2) < 1 else max_iter-1\n",
    "        rules = self.build_rules_of_cycles(max_iter)\n",
    "        \n",
    "        for rule in rules: \n",
    "            motif, query, select, degree = rule\n",
    "            degree_cycle = g.find(motif).filter(query)\n",
    "            \n",
    "            for sel in range(len(select)): \n",
    "                if sel==0:\n",
    "                    new_col = 'start'\n",
    "                    select_id = '{}.id'.format(select[sel])\n",
    "                else:\n",
    "                    new_col = 't{}_id'.format(sel+1)\n",
    "                    select_id = '{}.id'.format(select[sel])\n",
    "\n",
    "                select[sel] = select_id\n",
    "                degree_cycle = degree_cycle.select(select).withColumnRenamed('id', new_col)\n",
    "                select[sel] = new_col\n",
    "\n",
    "            degree_cycle_start = degree_cycle.select('start').distinct().withColumnRenamed('start', 'id')\n",
    "            degree_cycle_involved = degree_cycle.drop('start')\n",
    "            degree_cycle_involved = degree_cycle_involved.select(array([col(column) for column in degree_cycle_involved.columns])\\\n",
    "                                               .alias('id')).selectExpr('explode(id) as id').distinct()\n",
    "\n",
    "            startings = self.in_cycle(degree_cycle_start, degree) # != 0 if a transaction is the starting one of a cycle\n",
    "            print(\"adding cycles of degree {}...\".format(degree))\n",
    "            if not created_df: \n",
    "                starting_cycles = startings\n",
    "                intermediaries_cycles = degree_cycle_involved\n",
    "                created_df = True\n",
    "            else:\n",
    "                starting_cycles = starting_cycles.union(startings)\n",
    "                intermediaries_cycles = intermediaries_cycles.union(degree_cycle_involved)\n",
    "\n",
    "        starting_cycles = starting_cycles.groupBy('id').agg(\n",
    "            min(\"min_cycle\").alias(\"min_cycle\"),\n",
    "            max(\"max_cycle\").alias(\"max_cycle\")\n",
    "        )\n",
    "        \n",
    "        intermediaries_cycles = intermediaries_cycles.distinct()\n",
    "        \n",
    "        self.cycles = starting_cycles.join(intermediaries_cycles, 'id','fullouter').withColumn('involved', lit(1))\n",
    "            \n",
    "    def in_cycle(self, cycle_subset, degree):\n",
    "        cycle_subset = cycle_subset\\\n",
    "            .withColumn('min_cycle', lit(degree))\\\n",
    "            .withColumn('max_cycle', lit(degree))\n",
    "                \n",
    "        return cycle_subset        \n",
    "    \n",
    "########## END - CYCLE PATTERN ########## \n",
    "    def page_rank(self):\n",
    "        res = self.g.pageRank(resetProbability=0.15, tol=0.1) \n",
    "        edges = res.edges.select('src','dst','weight','id')\\\n",
    "            .withColumnRenamed('src','from_account').withColumnRenamed('dst','to_account')\\\n",
    "            .withColumn('weight_t', round(col('weight'),2))\\\n",
    "            .drop('weight')\n",
    "\n",
    "        vertices = res.vertices.withColumn('page_rank', round(col('pagerank'),2)).select('id','page_rank').withColumnRenamed('id','from_account')\n",
    "        edges = vertices.join(edges, 'from_account', 'right').withColumnRenamed('page_rank','fa_pagerank')\n",
    "        vertices = vertices.withColumnRenamed('from_account', 'to_account')\n",
    "        edges = vertices.join(edges, 'to_account', 'right').withColumnRenamed('page_rank','ta_pagerank')\\\n",
    "            .select('id','fa_pagerank','ta_pagerank','weight_t')\n",
    "\n",
    "        self.ids = self.ids.join(edges, 'id', 'left')\n",
    "\n",
    "    def join_ids(self):\n",
    "        self.ids = self.ids.drop('from_account','to_account')\n",
    "        self.ids = self.ids.join(self.forwards, 'id','left')\n",
    "        self.ids = self.ids.join(self.similar, 'id', 'left').join(self.same, 'id', 'left')\n",
    "        self.ids = self.ids.join(self.fans, 'id','left')\n",
    "\n",
    "    def join_dataframe(self):\n",
    "        self.df = self.df.join(self.ids, 'id', 'left').na.fill(value=0,subset=['exists_similar']).na.fill(value=1,subset=['fan_out_degree', 'fan_in_degree'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.parquet(\"src/datasets/HI_Small_train_set.parquet\", header=True)\n",
    "test = spark.read.parquet(\"src/datasets/HI_Small_test_set.parquet\", header=True)\n",
    "dataframe = spark.read.parquet(\"src/datasets/HI-Small_features.parquet\", header=True).select('id', 'same_account', 'same_bank', 'amount_difference', 'same_amounts', 'receiving_currency', 'payment_currency', 'same_currency', 'payment_format', 'is_laundering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = MyGraph(train)\n",
    "train_graph.get_forwards()\n",
    "train_graph.same_or_similar()\n",
    "train_graph.compute_fan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph.join_ids()\n",
    "\n",
    "train_graph.ids = train_graph.ids.drop('before_forward','forward','exists_same')\n",
    "train_graph.page_rank()\n",
    "\n",
    "train_graph.df = train_graph.df.join(train_graph.ids, 'id', 'left').na.fill(value=0,subset=['exists_similar'])\\\n",
    "    .na.fill(value=1,subset=['fan_out_degree', 'fan_in_degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleziono solo alcuni campi, dato che le features calcolate dal mygraph sono sporche (in quanto il dataset viene campionato troppo dividendolo in train e set)\n",
    "train = train_graph.df.select('id',\n",
    " 'week',\n",
    " 'day_of_month',\n",
    " 'hour',\n",
    " 'amount_received',\n",
    " 'amount_paid',\n",
    " 'day_of_week',\n",
    " 'transactions_same_hour_fa',\n",
    " 'transactions_same_day_fa',\n",
    " 'transactions_same_week_fa',\n",
    " 'transactions_same_hour_fata',\n",
    " 'transactions_same_day_fata',\n",
    " 'transactions_same_week_fata',\n",
    " 'from_account_inDegree',\n",
    " 'from_account_outDegree',\n",
    " 'to_account_inDegree',\n",
    " 'to_account_outDegree',\n",
    " 'exists_similar',\n",
    " 'fan_in_degree',\n",
    " 'fan_out_degree',\n",
    " 'fa_pagerank','ta_pagerank','weight_t')\n",
    "\n",
    "train = train.join(dataframe, 'id', 'left')\n",
    "#train.write.parquet(\"src/datasets/train_all_feats.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.write.parquet(\"src/datasets/train_page_rank.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph = MyGraph(test)\n",
    "test_graph.get_forwards()\n",
    "test_graph.same_or_similar()\n",
    "test_graph.compute_fan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/simonemalesardi/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/simonemalesardi/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/simonemalesardi/opt/anaconda3/envs/amd_sm2l/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_graph\u001b[39m.\u001b[39;49mjoin_ids()\n\u001b[1;32m      3\u001b[0m test_graph\u001b[39m.\u001b[39mids \u001b[39m=\u001b[39m test_graph\u001b[39m.\u001b[39mids\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mbefore_forward\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mexists_same\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m test_graph\u001b[39m.\u001b[39mpage_rank()\n",
      "Cell \u001b[0;32mIn[133], line 265\u001b[0m, in \u001b[0;36mMyGraph.join_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin_ids\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mfrom_account\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mto_account\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mids\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforwards, \u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilar, \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msame, \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfans, \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:2344\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2342\u001b[0m         on \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jseq([])\n\u001b[1;32m   2343\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(how, \u001b[39mstr\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mhow should be a string\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2344\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mjoin(other\u001b[39m.\u001b[39;49m_jdf, on, how)\n\u001b[1;32m   2345\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/amd_sm2l/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_graph.join_ids()\n",
    "\n",
    "test_graph.ids = test_graph.ids.drop('before_forward','forward','exists_same')\n",
    "test_graph.page_rank()\n",
    "\n",
    "test_graph.df = test_graph.df.join(test_graph.ids, 'id', 'left').na.fill(value=0,subset=['exists_similar'])\\\n",
    "    .na.fill(value=1,subset=['fan_out_degree', 'fan_in_degree'])\n",
    "\n",
    "\n",
    "# come per il caso sopra, seleziono solo alcuni campi, dato che le features calcolate dal mygraph sono sporche (in quanto il dataset viene campionato troppo dividendolo in train e set)\n",
    "test = test_graph.df.select('id',\n",
    " 'week',\n",
    " 'day_of_month',\n",
    " 'hour',\n",
    " 'amount_received',\n",
    " 'amount_paid',\n",
    " 'day_of_week',\n",
    " 'transactions_same_hour_fa',\n",
    " 'transactions_same_day_fa',\n",
    " 'transactions_same_week_fa',\n",
    " 'transactions_same_hour_fata',\n",
    " 'transactions_same_day_fata',\n",
    " 'transactions_same_week_fata',\n",
    " 'from_account_inDegree',\n",
    " 'from_account_outDegree',\n",
    " 'to_account_inDegree',\n",
    " 'to_account_outDegree',\n",
    " 'exists_similar',\n",
    " 'fan_in_degree',\n",
    " 'fan_out_degree',\n",
    " 'fa_pagerank','ta_pagerank','weight_t')\n",
    "\n",
    "test = test.join(dataframe, 'id', 'left')\n",
    "#test.write.parquet(\"src/datasets/test_all_feats.parquet\")\n",
    "test.write.parquet(\"src/datasets/test_page_rank.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_sm2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
