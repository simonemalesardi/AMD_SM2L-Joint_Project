{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/24 12:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from math import isnan\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graphframes import *\n",
    "\n",
    "########## START - PYSPARK ##########\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr, count, to_timestamp, monotonically_increasing_id, \\\n",
    "    desc, sum as _sum, min, max as _max, rand, when, \\\n",
    "    datediff, dayofmonth, weekofyear, month, year, hour, dayofweek, \\\n",
    "    unix_timestamp, array, lit\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer \n",
    "########## END - PYSPARK ##########\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"MoneyLaundering\") \\\n",
    "          .config(\"spark.driver.memory\", \"3g\") \\\n",
    "          .config(\"spark.executor.memory\", \"4g\") \\\n",
    "          .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "          .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "          .master(\"local[*]\") \\\n",
    "          .getOrCreate()\n",
    "          # con l'opzione \"local[*]\" per utilizzare tutti i core disponibili\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "train = spark.read.parquet(\"src/datasets/reduced_train_C.005.parquet\", header=True)\n",
    "test = spark.read.parquet(\"src/datasets/reduced_test_C.005.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "class SplitCriterion(Enum):\n",
    "    GINI = lambda probability: 1 - np.sum(probability ** 2)\n",
    "    MISCLASSIFICATION = lambda probability: 1 - np.max(probability)\n",
    "    SHANNON = lambda probability: -np.sum(probability * np.log2(probability))\n",
    "    ENTROPY = lambda probability: -np.sum(probability * np.log2(probability + 1e-10))\n",
    "    CHI_SQUARED = lambda probability: np.sum(((probability - np.mean(probability)) ** 2) / np.mean(probability))\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, info_gain=None, left=None, right=None, value=None):\n",
    "        \"\"\"\n",
    "        Create a Node object for a decision tree.\n",
    "\n",
    "        Args:\n",
    "            * feature (int): Index of the feature used for splitting at this node\n",
    "            * threshold (float): Threshold value used to split the data\n",
    "            * info_gain (float): Information gain achieved by the split\n",
    "            * left (Node): The left child node\n",
    "            * right (Node): The right child node\n",
    "            * value (int): Value assigned to the leaf node (class label: 0 or 1)\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.info_gain = info_gain\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  \n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class MyDecisionTree():\n",
    "    # educba.com/decision-tree-hyperparameters/\n",
    "    def __init__(self, max_n_features=None, split_criterion=SplitCriterion.GINI, \n",
    "                 depth_limit=None, min_samples_split=2, \n",
    "                 min_info_gain=0, max_thresholds=None, class_weights={}, \n",
    "                 random_state=None):       \n",
    "        \n",
    "        self.tree = None\n",
    "        self.node_count = 0\n",
    "        self.max_n_features = max_n_features\n",
    "        self.split_criterion = split_criterion\n",
    "\n",
    "        self.depth_limit = depth_limit\n",
    "        self.min_samples_split = min_samples_split\n",
    "        \n",
    "        self.min_info_gain = min_info_gain\n",
    "        self.max_thresholds = max_thresholds\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        if random_state is not None: \n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        # self.features_gain_dict = defaultdict(float)\n",
    "\n",
    "    ################################## TREE CONSTRUCTION ###################################\n",
    "    def fit(self, X, y):\n",
    "        self.node_count = 0\n",
    "        \n",
    "        X = self.__check_dataframe(X)\n",
    "        y = self.__check_dataframe(y, labels=True)\n",
    "        \n",
    "        self.tree = self.__grow_tree(X, y)\n",
    "    \n",
    "    def __check_dataframe(self, dataset, labels=False):\n",
    "        if(isinstance(dataset, pd.DataFrame)):\n",
    "            if labels: \n",
    "                dataset = np.array(dataset).flatten()\n",
    "            else: \n",
    "                dataset = np.array(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def __grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        self.node_count += 1\n",
    "\n",
    "        if (n_labels==1 or (self.depth_limit != None and depth >= self.depth_limit) or n_samples < self.min_samples_split):\n",
    "            leaf_value = Counter(y).most_common(1)[0][0] # get the majority class \n",
    "            return Node(value=leaf_value)        \n",
    "\n",
    "        best_feature, best_threshold, best_info_gain = self.__best_split(\n",
    "            X, y)\n",
    "\n",
    "        if(best_info_gain <= self.min_info_gain):\n",
    "            leaf_value = Counter(y).most_common(1)[0][0] # get the majority class \n",
    "            return Node(value=leaf_value)      \n",
    "          \n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_child = self.__grow_tree(\n",
    "            X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self.__grow_tree(\n",
    "            X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        if left_child.value != None and right_child.value != None and left_child.value == right_child.value:\n",
    "            return Node(value=left_child.value)        \n",
    "\n",
    "        return Node(feature=best_feature, threshold=best_threshold, info_gain=best_info_gain, left=left_child, right=right_child)\n",
    "    \n",
    "    def __best_split(self, X, y):\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        feature_count = X.shape[1]\n",
    "        \n",
    "        np.random.seed(self.random_state + self.node_count) \\\n",
    "            if self.random_state is not None else np.random.seed(None)\n",
    "            \n",
    "        feat_idxs = self.__calculate_num_selected_features(feature_count)\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            if self.max_thresholds is not None:\n",
    "                if self.max_thresholds > 0:\n",
    "                    thresholds = np.percentile(X[:, feat_idx], np.linspace(0, 100, self.max_thresholds))\n",
    "                else:\n",
    "                    raise ValueError(\"Max thresholds must be > 0\")\n",
    "            else:\n",
    "                unique_vals = np.unique(X[:, feat_idx])\n",
    "                thresholds = (unique_vals[1:] + unique_vals[:-1]) / 2\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                info_gain = self.__information_gain(\n",
    "                    X, y, feat_idx, threshold)\n",
    "                \n",
    "                # self.features_gain_dict[feat_idx] += info_gain\n",
    "                if info_gain > best_gain:\n",
    "                    best_gain = info_gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold, best_gain\n",
    "    \n",
    "    def __calculate_num_selected_features(self, feature_count):\n",
    "        if self.max_n_features is None:\n",
    "            num_selected_features = feature_count\n",
    "        elif self.max_n_features == \"sqrt\":\n",
    "            num_selected_features = int(math.sqrt(feature_count))\n",
    "        elif self.max_n_features == \"log2\":\n",
    "            num_selected_features = int(math.log2(feature_count))\n",
    "        elif isinstance(self.max_n_features, int):\n",
    "            num_selected_features = min(self.max_n_features, feature_count)\n",
    "        elif isinstance(self.max_n_features, float):\n",
    "            num_selected_features = int(feature_count * self.max_n_features)\n",
    "        \n",
    "        return np.random.permutation(feature_count)[:num_selected_features]\n",
    "\n",
    "    def __information_gain(self, X, y, feature, threshold):\n",
    "        left_entropy = self.__calculate_left_right_entropy(X, y, feature, threshold)\n",
    "        right_entropy = self.__calculate_left_right_entropy(X, y, feature, threshold, left=False)\n",
    "        \n",
    "        parent_entropy = self.__calculate_entropy(y)\n",
    "        info_gain = parent_entropy - (left_entropy + right_entropy)\n",
    "        return info_gain\n",
    "    \n",
    "    def __calculate_left_right_entropy(self, X, y, feature, threshold, left=True):\n",
    "        indexes = X[:, feature] <= threshold if left else X[:, feature] > threshold\n",
    "        labels = y[indexes]\n",
    "        entropy = self.__calculate_entropy(labels)\n",
    "        \n",
    "        weighted_entropy = (\n",
    "            len(labels) / len(y)) * entropy\n",
    "        \n",
    "        return weighted_entropy\n",
    "    \n",
    "    def __calculate_entropy(self, y):\n",
    "        distinct_y, len_distinct_y = np.unique(y, return_counts=True)\n",
    "        probability = len_distinct_y / len(y)\n",
    "\n",
    "        if self.class_weights and distinct_y:\n",
    "            weights = [self.class_weights.get(value, 1.0) for value in distinct_y]\n",
    "            probability = probability * np.array(weights)\n",
    "            \n",
    "        split_split_criterion = self.split_criterion(probability)\n",
    "\n",
    "        return split_split_criterion\n",
    "    \n",
    "    ################################## TREE CONSTRUCTION ###################################\n",
    "    \n",
    "    \n",
    "    ###################################### PREDICTION ######################################\n",
    "    def predict(self, X):\n",
    "        X = self.__check_dataframe(X)\n",
    "        self.y_pred = np.array([self.__explore_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def __explore_tree(self, sample, node):\n",
    "        \"\"\"\n",
    "        Check the current node based on the given sample.\n",
    "\n",
    "        If the node has a value, it's a leaf node and returns its label.\n",
    "        Otherwise, continue traversing the tree based on the sample's feature value.\n",
    "\n",
    "        Args:\n",
    "            sample (array): Input sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: If the node is a leaf, returns (label, True).\n",
    "                   If the node is not a leaf, returns (next_node, False).\n",
    "        \"\"\"                \n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if sample[node.feature] <= node.threshold:\n",
    "            return self.__explore_tree(sample, node.left)\n",
    "        else:\n",
    "            return self.__explore_tree(sample, node.right)\n",
    "    \n",
    "    ###################################### PREDICTION ######################################\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return \\\n",
    "            {'max_n_features': self.max_n_features,\n",
    "             'split_criterion': self.split_criterion,\n",
    "             'depth_limit': self.depth_limit,\n",
    "             'min_samples_split': self.min_samples_split,\n",
    "             'max_thresholds': self.max_thresholds,\n",
    "             'random_state': self.random_state}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    ######################################## REPORT ########################################\n",
    "    def get_tree_depth(self):\n",
    "        \"\"\"\n",
    "        Get the depth of the decision tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Depth of the decision tree.\n",
    "        \"\"\"\n",
    "        return self.__calculate_tree_depth(self.tree)\n",
    "    \n",
    "    def __calculate_tree_depth(self, node):\n",
    "        \"\"\"\n",
    "        Recursively calculate the depth of the tree starting from a given node.\n",
    "\n",
    "        Args:\n",
    "            node (Node): Current node being considered.\n",
    "\n",
    "        Returns:\n",
    "            int: Depth of the subtree rooted at the given node.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        if node.left is None and node.right is None:\n",
    "            return 1\n",
    "        \n",
    "        left_depth = self.__calculate_tree_depth(node.left)\n",
    "        right_depth = self.__calculate_tree_depth(node.right)\n",
    "        \n",
    "        return max(left_depth, right_depth) + 1\n",
    "\n",
    "    def compute_metrics(self, y_test, worker_id=None):\n",
    "        y_test = self.__check_dataframe(y_test, labels=True)\n",
    "        \n",
    "        tp = np.sum(np.logical_and(y_test == 1, self.y_pred == 1))\n",
    "        tn = np.sum(np.logical_and(y_test == 0, self.y_pred == 0))\n",
    "        fp = np.sum((y_test == 0) & (self.y_pred == 1))\n",
    "        fn = np.sum((y_test == 1) & (self.y_pred == 0))\n",
    "\n",
    "        accuracy = round((tp+tn)/len(y_test), 2)\n",
    "\n",
    "        recall = 0 if (tp == 0 and fn == 0) else round(tp/(tp+fn), 2)\n",
    "        precision = 0 if (tp == 0 and fp == 0) else round(tp/(tp+fp), 2)\n",
    "        f1_score = 0 if (recall == 0 and precision == 0) else round(\n",
    "            2*(precision*recall)/(precision+recall), 2)\n",
    "\n",
    "        metrics = f\" * Accuracy: {accuracy}\\n\"+\\\n",
    "            f\" * Recall: {recall}\\n\"+\\\n",
    "            f\" * Precision: {precision}\\n\"+\\\n",
    "            f\" * F1 score: {f1_score}\"\n",
    "        \n",
    "        self.confusion_matrix = [[tn, fp], [fn, tp]]\n",
    "        \n",
    "        if worker_id != None:\n",
    "            print(f\"WORKER {worker_id} METRICS:\\n\"+metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def get_report(self, y_test): \n",
    "        report = \"METRICS:\\n\" +  self.compute_metrics(y_test) + \\\n",
    "            \"\\n\\nREPORT:\\n\"+\\\n",
    "            f\" * Split criterion: {self.__split_function_name()}\\n\"+\\\n",
    "            f\" * Min samples split: {self.min_samples_split}\\n\"+\\\n",
    "            f\" * Max features: {self.max_n_features}\\n\"+\\\n",
    "            f\" * Max thresholds: {self.max_thresholds}\\n\"+\\\n",
    "            f\" * Depth limit: {self.depth_limit}\\n\"+\\\n",
    "            f\" * Depth reached: {self.get_tree_depth()}\\n\"\n",
    "            \n",
    "        print(report)\n",
    "        \n",
    "    def __split_function_name(self):\n",
    "        split_functions = {\n",
    "            SplitCriterion.GINI: \"gini\",\n",
    "            SplitCriterion.MISCLASSIFICATION: \"missclassification\",\n",
    "            SplitCriterion.SHANNON: \"shannon\",\n",
    "            SplitCriterion.ENTROPY: \"entropy\",\n",
    "            SplitCriterion.CHI_SQUARED: \"chi_square\"\n",
    "        }\n",
    "        return split_functions[self.split_criterion]\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        plt.imshow(self.confusion_matrix,\n",
    "                   interpolation='nearest', cmap=plt.cm.Reds)\n",
    "        plt.title('Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, ['0', '1'])\n",
    "        plt.yticks(tick_marks, ['0', '1'])\n",
    "\n",
    "        fmt = 'd'\n",
    "        thresh = np.max(self.confusion_matrix) / 2.\n",
    "        for i, j in itertools.product(range(len(self.confusion_matrix[0])), range(len(self.confusion_matrix[1]))):\n",
    "            plt.text(j, i, format(self.confusion_matrix[i][j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if self.confusion_matrix[i][j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        \n",
    "    ######################################## REPORT ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train2 = train.drop('id').toPandas()\n",
    "test2 = test.drop('id').toPandas()\n",
    "\n",
    "X_cols = [x for x in train2.columns if x != 'is_laundering' and x != 'amount_paid' and x != 'amount_received']\n",
    "X_train2, y_train2 = train2[X_cols], train2[['is_laundering']]\n",
    "X_test2, y_test2 = test2[X_cols], test2[['is_laundering']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonemalesardi/opt/anaconda3/envs/amd_sm2l/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/simonemalesardi/opt/anaconda3/envs/amd_sm2l/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "clf = MyDecisionTree()\n",
    "\n",
    "params = {\n",
    "    'max_n_features': [\"sqrt\", \"log2\", None, 0.1, 0.5, 10],\n",
    "    'split_criterion': [SplitCriterion.CHI_SQUARED, SplitCriterion.ENTROPY, SplitCriterion.GINI],\n",
    "    'depth_limit': [5, 10, 40, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_thresholds': [2, 10, None],\n",
    "    'random_state': [42, None]\n",
    "}\n",
    "gs = GridSearchCV(clf, params, verbose=1, scoring='accuracy')\n",
    "gs.fit(X_train2, y_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth_limit': 5,\n",
       " 'max_thresholds': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = gs.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.predict(X_test2)\n",
    "clf.get_report(y_test2)\n",
    "clf.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomForest():\n",
    "    def __init__(self, train_set, test_set, max_features = None):\n",
    "        self.n_estimators = spark.sparkContext.defaultParallelism\n",
    "        \n",
    "        self.distributed_train_set = train_set.drop('id', 'amount_paid','amount_received').repartition(self.n_estimators)\n",
    "        \n",
    "        self.test_set = test_set.drop('id','amount_paid','amount_received').toPandas()\n",
    "        self.X_cols = [x for x in self.distributed_train_set.columns if x != 'is_laundering']\n",
    "\n",
    "        self.max_features = max_features\n",
    "        self.trees = {}\n",
    "\n",
    "    def fit(self):\n",
    "        for id in range(self.n_estimators):\n",
    "            bootstrap_sample = self.distributed_train_set.sample(True, 1.0).toPandas()\n",
    "            X_train, y_train = bootstrap_sample[self.X_cols], bootstrap_sample[['is_laundering']]\n",
    "            \n",
    "            tree = MyDecisionTree() \n",
    "            tree.fit(X_train, y_train)\n",
    "            \n",
    "            self.trees[id] = tree\n",
    "\n",
    "    def predict(self):\n",
    "        predictions = []\n",
    "        X_test, self.y_test = self.test_set[self.X_cols], self.test_set[['is_laundering']]\n",
    "        if(isinstance(self.y_test, pd.DataFrame)):\n",
    "            self.y_test = np.array(self.y_test).flatten()\n",
    "            \n",
    "        for tree in self.trees.items(): \n",
    "            tree_id, tree_obj = tree[0], tree[1]\n",
    "            tree_obj.predict(X_test)\n",
    "            predictions.append(tree_obj.y_pred)\n",
    "\n",
    "        self.y_pred = np.array([Counter(col).most_common(1)[0][0] for col in zip(*predictions)])\n",
    "        \n",
    "    def get_tree_metrics(self):\n",
    "        for tree in self.trees.items(): \n",
    "            print(tree[0])\n",
    "            tree_id, tree_obj = tree[0], tree[1]  \n",
    "            tree_obj.compute_metrics(self.y_test, worker_id=tree_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = MyRandomForest(train_set=train, test_set=test)\n",
    "rf.fit()\n",
    "rf.predict()\n",
    "rf.get_tree_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_test, y_pred): \n",
    "    tp = np.sum(np.logical_and(y_test == 1, y_pred == 1))\n",
    "    tn = np.sum(np.logical_and(y_test == 0, y_pred == 0))\n",
    "    fp = np.sum((y_test == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_test == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = round((tp+tn)/len(y_test), 2)\n",
    "\n",
    "    recall = 0 if (tp == 0 and fn == 0) else round(tp/(tp+fn), 2)\n",
    "    precision = 0 if (tp == 0 and fp == 0) else round(tp/(tp+fp), 2)\n",
    "    f1_score = 0 if (recall == 0 and precision == 0) else round(\n",
    "        2*(precision*recall)/(precision+recall), 2)\n",
    "\n",
    "    metrics = f\" * Accuracy: {accuracy}\\n\"+\\\n",
    "        f\" * Recall: {recall}\\n\"+\\\n",
    "        f\" * Precision: {precision}\\n\"+\\\n",
    "        f\" * F1 score: {f1_score}\"\n",
    "    print(metrics)\n",
    "    \n",
    "    return [[tn, fp], [fn, tp]]\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    plt.imshow(confusion_matrix,\n",
    "                interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['0', '1'])\n",
    "    plt.yticks(tick_marks, ['0', '1'])\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = np.max(confusion_matrix) / 2.\n",
    "    for i, j in itertools.product(range(len(confusion_matrix[0])), range(len(confusion_matrix[1]))):\n",
    "        plt.text(j, i, format(confusion_matrix[i][j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if confusion_matrix[i][j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = compute_metrics(rf.y_test, rf.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_sm2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
