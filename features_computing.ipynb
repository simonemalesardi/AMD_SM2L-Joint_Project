{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/19 14:48:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from math import isnan\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graphframes import *\n",
    "\n",
    "########## START - PYSPARK ##########\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr, count, to_timestamp, monotonically_increasing_id, \\\n",
    "    desc, sum as _sum, min, max, rand, when, \\\n",
    "    datediff, dayofmonth, weekofyear, month, year, hour, dayofweek, \\\n",
    "    unix_timestamp, array, lit, round\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer \n",
    "########## END - PYSPARK ##########\n",
    "\n",
    "# Count available cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "# In this case the amount of executors will be equal to the amount of cores\n",
    "instances = cores\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"AMD-SM2L Joint Project\") \\\n",
    "          .config(\"spark.driver.memory\", \"3g\") \\\n",
    "          .config(\"spark.executor.memory\", \"4g\") \\\n",
    "          .config(\"spark.executor.instances\", cores) \\\n",
    "          .config(\"spark.executor.cores\", cores//instances) \\\n",
    "          .config(\"spark.sql.shuffle.partitions\", cores) \\\n",
    "          .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "          .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "dataframe = spark.read.parquet(\"src/datasets/small_with_ids.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dato il dataset iniziale: \n",
    "- calcolo le feature indipendenti nel dataset completo\n",
    "- divido il df in train e test\n",
    "- calcolo le feature dipendenti\n",
    "- aggiorno train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureManager:\n",
    "    def __init__(self, dataframe):\n",
    "        # Percorso del file Parquet\n",
    "        self.origDF = dataframe\n",
    "        \n",
    "    def compute_features_of_whole_df(self):\n",
    "        path = 'src/datasets/small_features.parquet'\n",
    "        if not Path(path).exists():\n",
    "            self.df_features = self.origDF\\\n",
    "                .withColumnRenamed('receiving_currency', 'rec_cur')\\\n",
    "                .withColumnRenamed('payment_currency', 'pay_cur')\\\n",
    "                .withColumnRenamed('payment_format', 'pay_for')\\\n",
    "\n",
    "            currencies = self.df_features.select('rec_cur').distinct().union(self.df_features.select('pay_cur').distinct())\n",
    "            \n",
    "            currency = StringIndexer(inputCol='rec_cur', outputCol='receiving_currency')\n",
    "            payment_format = StringIndexer(inputCol='pay_for', outputCol='payment_format')\n",
    "            \n",
    "            rec_currency_model = currency.fit(currencies)\n",
    "            self.df_features = rec_currency_model.transform(self.df_features)\n",
    "            pay_currency_model = rec_currency_model.setInputCol('pay_cur').setOutputCol('payment_currency')\n",
    "            self.df_features = pay_currency_model.transform(self.df_features)\n",
    "            \n",
    "            payment_format_model = payment_format.fit(self.df_features)\n",
    "            self.df_features = payment_format_model.transform(self.df_features)\n",
    "            self.ach_mapping = {v: k for k, v in dict(enumerate(payment_format_model.labels)).items()}\n",
    "            \n",
    "            column_order = ['id', 'timestamp',\n",
    "                            'from_account','to_account','same_account',\n",
    "                            'from_bank','to_bank','same_bank',\n",
    "                            'amount_received','amount_paid','amount_difference','same_amounts',\n",
    "                            'receiving_currency','payment_currency','same_currency',\n",
    "                            'payment_format', 'is_laundering']\n",
    "            \n",
    "            self.df_features = self.df_features\\\n",
    "                    .withColumn('same_bank', (col('from_bank')==col('to_bank')).cast('integer'))\\\n",
    "                    .withColumn('same_account', (col('from_account')==col('to_account')).cast('integer'))\\\n",
    "                    .withColumn('same_currency', (col('receiving_currency')==col('payment_currency')).cast('integer'))\\\n",
    "                    .withColumn('same_amounts', (col('amount_received')==col('amount_paid')).cast('integer'))\\\n",
    "                    .withColumn('amount_difference', (col('amount_paid')-col('amount_received')))\\\n",
    "                    .select(column_order)\n",
    "\n",
    "            self.df_features.write.parquet(path)\n",
    "        else:\n",
    "            self.df_features = spark.read.parquet(path, header=True)\n",
    "\n",
    "    def compute_timestamp_features(self, train=True):\n",
    "        df = self.train_df if train else self.test_df\n",
    "\n",
    "        df = df\\\n",
    "            .withColumn('week', weekofyear(\"timestamp\"))\\\n",
    "            .withColumn('day_of_month', dayofmonth(\"timestamp\"))\\\n",
    "            .withColumn('day_of_week', dayofweek(\"timestamp\"))\\\n",
    "            .withColumn('hour', hour(\"timestamp\"))\n",
    "\n",
    "        hour_FA = df.groupBy('from_account','hour').agg(count('*').alias('transactions_same_hour_fa'))\n",
    "        day_of_month_FA = df.groupBy('from_account','day_of_month').agg(count('*').alias('transactions_same_day_fa'))\n",
    "        week_FA = df.groupBy('from_account','week').agg(count('*').alias('transactions_same_week_fa'))\n",
    "        \n",
    "        hour_FA_TA = df.groupBy('from_account','to_account', 'hour').agg(count('*').alias('transactions_same_hour_fata'))\n",
    "        day_of_month_FA_TA = df.groupBy('from_account','to_account','day_of_month').agg(count('*').alias('transactions_same_day_fata'))\n",
    "        week_FA_TA = df.groupBy('from_account','to_account','week').agg(count('*').alias('transactions_same_week_fata'))\n",
    "        \n",
    "        df = df\\\n",
    "            .join(hour_FA, ['from_account', 'hour'], 'left')\\\n",
    "            .join(day_of_month_FA, ['from_account', 'day_of_month'], 'left')\\\n",
    "            .join(week_FA, ['from_account', 'week'], 'left')\\\n",
    "            .join(hour_FA_TA, ['from_account', 'to_account', 'hour'], 'left')\\\n",
    "            .join(day_of_month_FA_TA, ['from_account', 'to_account', 'day_of_month'], 'left')\\\n",
    "            .join(week_FA_TA, ['from_account', 'to_account', 'week'], 'left')\n",
    "        \n",
    "        if train: \n",
    "            self.train_df = df\n",
    "        else: \n",
    "            self.test_df = df\n",
    "\n",
    "    def drop_columns(self):\n",
    "        cols = ('rec_cur','pay_cur','pay_for','from_bank','to_bank','from_account','to_account')\n",
    "        self.dataframe = self.dataframe.drop(*cols)\n",
    "\n",
    "    def split_original_datasets(self):\n",
    "        # classes are unbalanced, so we need to take a similar number of laundering and non laundering transactions:\n",
    "        # in order to do that the dataset is filtered taking randomically a limited number of non laundering transactions \n",
    "        # and then laundering transactions are added \n",
    "        self.origDF = self.origDF.drop('from_bank','to_bank')\n",
    "        launderings = self.origDF.filter('is_laundering==1')\n",
    "\n",
    "        print('\\nSplitting dataframe into train and test set...')\n",
    "        sample_non_laundering = self.origDF.filter('is_laundering==0').orderBy(rand()).limit(launderings.count())\n",
    "        sample_dataset = sample_non_laundering.union(launderings.filter('is_laundering==1'))\n",
    "\n",
    "        self.train_df, test_df = sample_dataset.randomSplit([0.8, 0.2])\n",
    "\n",
    "        subtracted = self.origDF.subtract(sample_dataset)\n",
    "        self.test_df = test_df.union(subtracted)   \n",
    "        print('Dataframe splitted')\n",
    "\n",
    "    def split_with_undersampling(self):\n",
    "        self.df_features = self.df_features.drop('from_bank','to_bank')\n",
    "        launderings = self.df_features.filter('is_laundering==1')\n",
    "        non_launderings = self.df_features.filter('is_laundering==0')\n",
    "        \n",
    "        train_l, test_l = launderings.randomSplit([0.8, 0.2])\n",
    "        launderings_count = launderings.count()\n",
    "        non_launderings_count = non_launderings.count()\n",
    "        \n",
    "        percentage_launderings = 100*launderings_count/(launderings_count + non_launderings_count) # 0.1%\n",
    "        \n",
    "        sample_non_laundering = non_launderings.orderBy(rand()).limit(int(non_launderings_count/100))\n",
    "        train_nl, test_nl = sample_non_laundering.randomSplit([0.8, 0.2])\n",
    "\n",
    "        self.train_df = train_l.union(train_nl)\n",
    "        self.test_df = test_l.union(test_nl)\n",
    "\n",
    "        self.train_df.select('id').join(self.origDF, 'id', 'left').write.parquet('src/datasets/train_orig_features.parquet')\n",
    "        self.test_df.select('id').join(self.origDF, 'id', 'left').write.parquet('src/datasets/test_orig_features.parquet')\n",
    "\n",
    "    def save_dataframes(self, path):\n",
    "        self.train_df.write.parquet(path.format('train_set'))\n",
    "        self.test_df.write.parquet(path.format('test_set'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "manager = FeatureManager(dataframe)\n",
    "manager.compute_features_of_whole_df()\n",
    "manager.split_with_undersampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and computation over train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "manager.compute_timestamp_features(True)\n",
    "manager.compute_timestamp_features(False)\n",
    "manager.save_dataframes('src/datasets/{}_noGF.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):     \n",
    "    columns = df.columns\n",
    "    assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "    feature_vector = assembler.transform(df)\n",
    "\n",
    "    matrix = Correlation.corr(feature_vector, \"features\").collect()[0][0]\n",
    "    corr_matrix = np.array(matrix.toArray())\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(corr_matrix, xticklabels=columns, yticklabels=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = manager.train_df.select(['week', 'day_of_month', 'hour', 'same_account', 'same_bank', 'amount_received', 'amount_paid', \\\n",
    "    'amount_difference', 'same_amounts', 'receiving_currency', 'payment_currency', 'same_currency', 'payment_format',\n",
    "    'is_laundering', 'day_of_week', 'transactions_same_hour_fa', 'transactions_same_day_fa', 'transactions_same_week_fa',\n",
    "    'transactions_same_hour_fata', 'transactions_same_day_fata', 'transactions_same_week_fata'])\n",
    "    \n",
    "correlation_matrix(train_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to understand the structure of the different patterns in order to identify further features\n",
    "\n",
    "\n",
    "<img src=\"src/images/patterns.png\" style=\"width: 600px\">\n",
    "\n",
    "\n",
    "In order to do that, I thought that the best solution was to process the dataset using GraphFrames, a package for Apache Spark which provides DataFrame-based Graph.\n",
    "\n",
    "Looking at the image below, it would be beneficial to process certain features for each node in the graph to gain valuable insights into the transactions:\n",
    "1. **Compute the number of in-out edges (fan-in, fan-out)** <br>\n",
    "    A transaction involves an exchange between two accounts, and it would be valuable to calculate the connection degrees for each account:\n",
    "    * In-out degrees for the sender account\n",
    "    * In-out degrees for the receiver account\n",
    "    <br><br>\n",
    "2. **Identify intermediary transactions (scatter-gather)** <br>\n",
    "    By analyzing the flow of transactions, we can identify intermediary transactions. These are transactions that act as intermediaries, facilitating the movement of funds between multiple accounts\n",
    "    <br><br>\n",
    "3. **Detect forwarding transactions** <br>\n",
    "    An account receives a sum of money and then forwards it to another account\n",
    "    <br><br>\n",
    "4. **Check for intermediate transactions between two transactions** <br>\n",
    "    We can check if certain transactions act as intermediaries between two other transactions\n",
    "    <br><br>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.parquet('src/datasets/train_set_noGF.parquet')\n",
    "test = spark.read.parquet('src/datasets/test_set_noGF.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig_features = train.select('id').join(dataframe, 'id', 'left')\n",
    "test_orig_features = test.select('id').join(dataframe, 'id', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGraph:\n",
    "    # create the graph using the vertices and edges found in the dataset taken into account (train or test)\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.ids = self.df.select('id','from_account','to_account')\n",
    "        self.vertices, self.edges, self.g = self.create_graph()\n",
    "        self.compute_inOut_degrees()\n",
    "\n",
    "    def create_graph(self, init=True):\n",
    "        vertices = self.df.select(\"from_account\")\\\n",
    "                            .withColumnRenamed('from_account', 'id')\\\n",
    "                            .union(self.ids.select(\"to_account\"))\\\n",
    "                            .distinct()\n",
    "        if init:\n",
    "            edges = self.df.withColumnRenamed('from_account', 'src')\\\n",
    "                .withColumnRenamed('to_account', 'dst')\n",
    "        else:\n",
    "            edges = self.df.withColumnRenamed('from_account', 'src')\\\n",
    "                .withColumnRenamed('to_account', 'dst').filter('from_account!=to_account and receiving_currency==payment_currency and payment_format=\"ACH\"')\\\n",
    "                .select('id','timestamp','src','dst','payment_currency','payment_format')\n",
    "\n",
    "        g = GraphFrame(vertices, edges)\n",
    "        return vertices, edges, g\n",
    "\n",
    "    def compute_inOut_degrees(self):\n",
    "        # for each account, it computes the number of ingoing and outgoing transactions \n",
    "        vertexInDegrees = self.g.inDegrees\n",
    "        vertexOutDegrees = self.g.outDegrees\n",
    "        vertices = vertexInDegrees.join(vertexOutDegrees, 'id', 'fullouter').fillna(0)\n",
    "        \n",
    "        vertices = vertices.withColumnRenamed('id', 'from_account')\n",
    "        self.ids = self.ids.alias('df').join(vertices.alias('vertices'), 'from_account', 'left')\\\n",
    "                        .withColumnRenamed('inDegree','from_account_inDegree')\\\n",
    "                        .withColumnRenamed('outDegree','from_account_outDegree')\n",
    "\n",
    "        vertices = vertices.withColumnRenamed('from_account', 'to_account')\n",
    "        self.ids = self.ids.join(vertices.alias('vertices'), 'to_account', 'left')\\\n",
    "                    .withColumnRenamed('inDegree','to_account_inDegree')\\\n",
    "                    .withColumnRenamed('outDegree','to_account_outDegree')\n",
    "\n",
    "    def get_forwards(self):\n",
    "        # it consists in getting all transactions in which the receiver of the transaction \n",
    "        # sends the same amount of received money to another account\n",
    "        # OUTPUT: id of inolved transactions where:\n",
    "        # - before_forward: 1 if a transaction is that one before a secondly forwarding transaction\n",
    "        # - forward: 1 if a transaction is that one that makes the forward\n",
    "\n",
    "        motif = \"(a)-[e]->(b); (b)-[e2]->(c)\"\n",
    "        forwards = self.g.find(motif).filter(\"e.amount_received == e2.amount_paid and e.timestamp <= e2.timestamp and a!=b and b!=c\")\n",
    "    \n",
    "        before_forward = forwards.select(col('e.id').alias('id'))\\\n",
    "            .distinct()\\\n",
    "            .withColumn('before_forward',lit(1))\n",
    "        # distinct: I can use it, or I can count how many times the id is involved\n",
    "        forward = forwards.select(col('e2.id').alias('id'))\\\n",
    "            .distinct()\\\n",
    "            .withColumn('forward',lit(1))\n",
    "        # distinct: I can use it, or I can count how many times the id is involved\n",
    "    \n",
    "        self.forwards = before_forward.join(forward, 'id','left')#.na.fill(value=0,subset=['before_forward','forward'])\n",
    "     \n",
    "    def same_or_similar(self):\n",
    "        # it search if for each transaction there is:\n",
    "        # - another transaction with the same attributes, except the amounts (exists_same)\n",
    "        # - another transaction with similar attributes, except the timestamps and amounts (exists_similar)\n",
    "        motif = \"(a)-[t1]->(b); (a)-[t2]->(b)\"\n",
    "\n",
    "        same_where = 't1.timestamp == t2.timestamp and \\\n",
    "                        t1.payment_currency == t2.payment_currency and \\\n",
    "                        t1.receiving_currency == t2.receiving_currency and \\\n",
    "                        t1.payment_format == t2.payment_format and \\\n",
    "                        t1.amount_paid != t2.amount_paid and \\\n",
    "                        t1.id != t2.id'\n",
    "        \n",
    "        self.same = self.g.find(motif).filter(same_where).select('t1.id').withColumn('exists_same',lit(1)).distinct()\n",
    "\n",
    "        similar_where = 't1.timestamp != t2.timestamp and \\\n",
    "                        t1.payment_currency == t2.payment_currency and \\\n",
    "                        t1.receiving_currency == t2.receiving_currency and \\\n",
    "                        t1.payment_format == t2.payment_format and \\\n",
    "                        t1.amount_paid != t2.amount_paid'\n",
    "\n",
    "        \n",
    "        self.similar = self.g.find(motif).filter(similar_where).select('t1.id').withColumn('exists_similar',lit(1)).distinct()\n",
    "########## START - FAN PATTERN ##########\n",
    "    def compute_fan_in(self):\n",
    "        \"\"\"\n",
    "            as explained in undestand_pattern.ipynb it is useful to compute the following feature: \n",
    "            - for each to_account, the number of incoming nodes to the same bank and all in node must have the same: \n",
    "                * receiving_currency \n",
    "                * payment_currency\n",
    "                * payment_format\n",
    "                * there must be at most 4 days between the first transaction and the last in the series\n",
    "        \"\"\"\n",
    "        motif = \"(a)-[t1]->(b); (c)-[t2]->(b)\"\n",
    "        \n",
    "        fan_in_query = 'abs(datediff(t1.timestamp, t2.timestamp)) <= 4 and \\\n",
    "                    t1.payment_currency == t2.payment_currency and \\\n",
    "                    t1.receiving_currency == t2.receiving_currency and \\\n",
    "                    t1.payment_format == t2.payment_format'\n",
    "                \n",
    "        fan_in = self.g.find(motif).filter(fan_in_query).select('a', 'b', 't1')\n",
    "        fan_in = fan_in.groupBy('a', 'b', 't1').count().select('t1.id',col('count').alias('fan_in_degree'))\n",
    "\n",
    "        return fan_in\n",
    "\n",
    "    def compute_fan_out(self):\n",
    "        \"\"\"\n",
    "            as explained in undestand_pattern.ipynb it is useful to compute the following feature: \n",
    "            - for each from_account, the number of outgoing nodes to the same bank and all in node must have the same: \n",
    "                * payment_format\n",
    "                * there must be at most 4 days between the first transaction and the last in the series\n",
    "            \n",
    "            in order to handle the big amount of data, data are firstly filtered:\n",
    "            - self transaction (from_account == to_account) doesn't exist in the same fan-out\n",
    "            - two similar transactions (t1(from_account, to_account) == t2(from_account, to_account) ) don't exist in the same fan-out \n",
    "            - fan-outs have ACH payment_format\n",
    "        \"\"\"\n",
    "        _, _, g = self.create_graph(False)\n",
    "\n",
    "        motif = \"(a)-[t1]->(b); (a)-[t2]->(c)\"\n",
    "        \n",
    "        fan_out_query = 'abs(datediff(t1.timestamp, t2.timestamp)) <= 4 and \\\n",
    "                        a != b and a != c and b != c and\\\n",
    "                        t1.id != t2.id'\n",
    "                \n",
    "        fan_out = g.find(motif).filter(fan_out_query).select('a', 'b', 'c', 't1.id')\n",
    "        fan_out = fan_out.groupBy('a','b','c','id').count()\n",
    "        fan_out = fan_out.groupBy('id').agg(count('*').alias('fan_out_degree')).select('id', 'fan_out_degree').withColumn('fan_out_degree', col('fan_out_degree')+1)\n",
    "        \n",
    "        return fan_out\n",
    "    \n",
    "    def compute_fan(self):\n",
    "        fan_in = self.compute_fan_in()\n",
    "        fan_out = self.compute_fan_out()  \n",
    "        \n",
    "        self.fans = fan_in.join(fan_out, 'id', 'fullouter')\n",
    "########## END - FAN PATTERN ##########\n",
    "\n",
    "########## START - CYCLE PATTERN ##########\n",
    "    def generate_combinations(self,accounts):\n",
    "        accounts = set(accounts)\n",
    "        conditions = []\n",
    "        for acc in accounts: \n",
    "            for acc2 in accounts:\n",
    "                if acc!=acc2:\n",
    "                    if ((acc, acc2) not in conditions) and ((acc2, acc) not in conditions):\n",
    "                        conditions.append((acc,acc2))\n",
    "\n",
    "        return ' and '.join(conditions[k][0]+'!='+conditions[k][1] for k in range(len(conditions)))\n",
    "\n",
    "    def build_rules_of_cycles(self, max_iter):\n",
    "        alphabet = list(map(chr, range(97, 123)))\n",
    "        start = 2\n",
    "\n",
    "        rules = []\n",
    "        \n",
    "        for i in range(start-1, max_iter+1):\n",
    "            full_rule = []\n",
    "            single_query = []\n",
    "            transactions = []\n",
    "            receiving_accounts = []\n",
    "            select = []\n",
    "            accounts = []\n",
    "            for j in range(0, i+1):\n",
    "                receiving_account = alphabet[j+1] if j < i else alphabet[0]\n",
    "                receiving_accounts.append(receiving_account)\n",
    "\n",
    "                single_transaction = 't{}'.format(j+1)\n",
    "                transactions.append(single_transaction)\n",
    "\n",
    "                starting_account = alphabet[j]\n",
    "\n",
    "                single_rule = \"({})-[{}]->({})\".format(starting_account, single_transaction, receiving_account)\n",
    "                accounts.append(starting_account)\n",
    "                accounts.append(receiving_account)\n",
    "\n",
    "                full_rule.append(single_rule)    \n",
    "                select.append(single_transaction)\n",
    "            \n",
    "            single_query.append(' and '.join(transactions[k] + '.timestamp <= ' + transactions[k+1] + '.timestamp' for k in range(len(transactions) - 1)))\n",
    "            single_query.append(self.generate_combinations(accounts))\n",
    "        \n",
    "            rules.append(('; '.join(full_rule), ' and '.join(single_query), select, (i+1)))\n",
    "\n",
    "        return rules\n",
    "\n",
    "    def find_cycles(self, max_iter):\n",
    "        # this method obtains 3 features: \n",
    "        # - min_cycle: != 0 if the transaction is the starting one of a cycle\n",
    "        # - max_cycle: != 0 if the transaction is the starting one of a cycle (== min_cycle if there's only that kind of degree)\n",
    "        # - involved: 1 if the transaction is involved in a cycle, 0 otherwise\n",
    "        _, _, g = self.create_graph(False)\n",
    "        \n",
    "        created_df = False\n",
    "\n",
    "        max_iter = 1 if (max_iter-2) < 1 else max_iter-1\n",
    "        rules = self.build_rules_of_cycles(max_iter)\n",
    "        \n",
    "        for rule in rules: \n",
    "            motif, query, select, degree = rule\n",
    "            degree_cycle = g.find(motif).filter(query)\n",
    "            \n",
    "            for sel in range(len(select)): \n",
    "                if sel==0:\n",
    "                    new_col = 'start'\n",
    "                    select_id = '{}.id'.format(select[sel])\n",
    "                else:\n",
    "                    new_col = 't{}_id'.format(sel+1)\n",
    "                    select_id = '{}.id'.format(select[sel])\n",
    "\n",
    "                select[sel] = select_id\n",
    "                degree_cycle = degree_cycle.select(select).withColumnRenamed('id', new_col)\n",
    "                select[sel] = new_col\n",
    "\n",
    "            degree_cycle_start = degree_cycle.select('start').distinct().withColumnRenamed('start', 'id')\n",
    "            degree_cycle_involved = degree_cycle.drop('start')\n",
    "            degree_cycle_involved = degree_cycle_involved.select(array([col(column) for column in degree_cycle_involved.columns])\\\n",
    "                                               .alias('id')).selectExpr('explode(id) as id').distinct()\n",
    "\n",
    "            startings = self.in_cycle(degree_cycle_start, degree) # != 0 if a transaction is the starting one of a cycle\n",
    "            print(\"adding cycles of degree {}...\".format(degree))\n",
    "            if not created_df: \n",
    "                starting_cycles = startings\n",
    "                intermediaries_cycles = degree_cycle_involved\n",
    "                created_df = True\n",
    "            else:\n",
    "                starting_cycles = starting_cycles.union(startings)\n",
    "                intermediaries_cycles = intermediaries_cycles.union(degree_cycle_involved)\n",
    "\n",
    "        starting_cycles = starting_cycles.groupBy('id').agg(\n",
    "            min(\"min_cycle\").alias(\"min_cycle\"),\n",
    "            max(\"max_cycle\").alias(\"max_cycle\")\n",
    "        )\n",
    "        \n",
    "        intermediaries_cycles = intermediaries_cycles.distinct()\n",
    "        \n",
    "        self.cycles = starting_cycles.join(intermediaries_cycles, 'id','fullouter').withColumn('involved', lit(1))\n",
    "            \n",
    "    def in_cycle(self, cycle_subset, degree):\n",
    "        cycle_subset = cycle_subset\\\n",
    "            .withColumn('min_cycle', lit(degree))\\\n",
    "            .withColumn('max_cycle', lit(degree))\n",
    "                \n",
    "        return cycle_subset        \n",
    "    \n",
    "########## END - CYCLE PATTERN ########## \n",
    "    def page_rank(self):\n",
    "        res = self.g.pageRank(resetProbability=0.15, tol=0.1) \n",
    "        edges = res.edges.select('src','dst','weight','id')\\\n",
    "            .withColumnRenamed('src','from_account').withColumnRenamed('dst','to_account')\\\n",
    "            .withColumn('weight_t', round(col('weight'),2))\\\n",
    "            .drop('weight')\n",
    "\n",
    "        vertices = res.vertices.withColumn('page_rank', round(col('pagerank'),2)).select('id','page_rank').withColumnRenamed('id','from_account')\n",
    "        edges = vertices.join(edges, 'from_account', 'right').withColumnRenamed('page_rank','fa_pagerank')\n",
    "        vertices = vertices.withColumnRenamed('from_account', 'to_account')\n",
    "        edges = vertices.join(edges, 'to_account', 'right').withColumnRenamed('page_rank','ta_pagerank')\\\n",
    "            .select('id','fa_pagerank','ta_pagerank','weight_t')\n",
    "\n",
    "        self.ids = self.ids.join(edges, 'id', 'left')\n",
    "\n",
    "    def join_features(self):\n",
    "        self.ids = self.ids.drop('from_account','to_account')\n",
    "        self.ids = self.ids.join(self.forwards, 'id','left')\n",
    "        self.ids = self.ids.join(self.similar, 'id', 'left').join(self.same, 'id', 'left')\n",
    "        self.ids = self.ids.join(self.fans, 'id','left')\n",
    "\n",
    "        self.df = self.df.join(self.ids, 'id', 'left').na.fill(value=0,subset=['exists_similar','forward','before_forward','exists_same']).na.fill(value=1,subset=['fan_out_degree', 'fan_in_degree'])\n",
    "\n",
    "    def final_permutation_and_save(self, df_, train=True):\n",
    "        self.df = self.df.drop('from_bank','from_account','timestamp','to_bank','to_account',\\\n",
    "            'receiving_currency','payment_currency','payment_format','is_laundering',\\\n",
    "            'amount_paid','amount_received')\n",
    "        \n",
    "        df_ = df_.drop('from_account','to_account','timestamp')\n",
    "        self.df = self.df.join(df_, 'id', 'left')\n",
    "\n",
    "        self.df.write.parquet('src/datasets/{}_set_withGF.parquet'.format('train' if train else 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_graph = MyGraph(train_orig_features)\n",
    "train_graph.get_forwards()\n",
    "train_graph.same_or_similar()\n",
    "train_graph.compute_fan()\n",
    "train_graph.page_rank()\n",
    "train_graph.ids = train_graph.ids.cache()\n",
    "train_graph.join_features() \n",
    "train_graph.final_permutation_and_save(train)\n",
    "train_graph.ids.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonemalesardi/spark/python/pyspark/sql/dataframe.py:169: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/Users/simonemalesardi/spark/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, from_account_inDegree: int, from_account_outDegree: int, to_account_inDegree: int, to_account_outDegree: int, fa_pagerank: double, ta_pagerank: double, weight_t: double, before_forward: int, forward: int, exists_similar: int, exists_same: int, fan_in_degree: bigint, fan_out_degree: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph = MyGraph(test_orig_features)\n",
    "test_graph.get_forwards()\n",
    "test_graph.same_or_similar()\n",
    "test_graph.compute_fan()\n",
    "test_graph.page_rank()\n",
    "test_graph.ids = test_graph.ids.cache()\n",
    "test_graph.join_features() \n",
    "test_graph.final_permutation_and_save(test, False)\n",
    "test_graph.ids.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train = spark.read.parquet('src/datasets/train_set_withGF.parquet')\n",
    "X_train, y_train = train.drop('id','is_laundering'), train.select('is_laundering')\n",
    "\n",
    "X_train_pd = X_train.toPandas()\n",
    "y_train_pd = y_train.toPandas()\n",
    "\n",
    "test = spark.read.parquet('src/datasets/test_set_withGF.parquet')\n",
    "X_test, y_test = test.drop('id','is_laundering'), test.select('is_laundering')\n",
    "\n",
    "X_test_pd = X_test.toPandas()\n",
    "y_test_pd = y_test.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonemalesardi/opt/anaconda3/envs/amd_sm2l/lib/python3.8/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_pd)\n",
    "\n",
    "sel_ = SelectFromModel(\n",
    "    LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))\n",
    "sel_.fit(scaler.transform(X_train_pd), y_train_pd)\n",
    "\n",
    "removed_feats = X_train_pd.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats\n",
    "\n",
    "X_train_selected = sel_.transform(scaler.transform(X_train_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['from_account_outDegree', 'to_account_inDegree', 'fa_pagerank',\n",
       "       'forward', 'amount_difference', 'payment_currency',\n",
       "       'transactions_same_week_fa'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel2_ = SelectFromModel(Lasso(alpha=0.001, random_state=10))\n",
    "sel2_.fit(scaler.transform(X_train_pd), y_train_pd)\n",
    "removed_feats = X_train_pd.columns[(sel2_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approccio di regressione logistica seguito dal sito\n",
    "\n",
    "https://vishaljadhav96.medium.com/feature-selection-with-lasso-regression-645308c78b5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuando lo scaling delle features l'algoritmo converge più velocemente e toglie anche il rischio di non convergere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols = X_train.columns \n",
    "cols_to_be_removed = ['before_forward', 'forward','exists_similar','exists_same',\n",
    "                 'same_account', 'same_bank','amount_received','amount_paid','amount_difference']\n",
    "\n",
    "cols_to_scale = [col for col in cols if col not in cols_to_be_removed]\n",
    "\n",
    "# operazione di normalizzazione necessaria prima di applicare il modello\n",
    "scaler = StandardScaler()\n",
    "X_train_pd[cols_to_scale] = scaler.fit_transform(X_train_pd[cols_to_scale])\n",
    "X_test_pd[cols_to_scale] = scaler.fit_transform(X_test_pd[cols_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuando lo scaling di tutte le feature, con il codice sottostante, l'algoritmo converge ancor più velocemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # operazione di normalizzazione necessaria prima di applicare il modello\n",
    "# scaler = StandardScaler()\n",
    "# X_train_pd[X_train_pd.columns] = scaler.fit_transform(X_train_pd[X_train_pd.columns])\n",
    "# X_test_pd[X_test_pd.columns] = scaler.fit_transform(X_test_pd[X_test_pd.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Non-Zero Coeffs</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.935202</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>0.449407</td>\n",
       "      <td>0.570602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.935115</td>\n",
       "      <td>0.780063</td>\n",
       "      <td>0.449407</td>\n",
       "      <td>0.570272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.935115</td>\n",
       "      <td>0.780063</td>\n",
       "      <td>0.449407</td>\n",
       "      <td>0.570272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.935289</td>\n",
       "      <td>0.782540</td>\n",
       "      <td>0.449407</td>\n",
       "      <td>0.570932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.935726</td>\n",
       "      <td>0.785150</td>\n",
       "      <td>0.453054</td>\n",
       "      <td>0.574566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0500</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.935726</td>\n",
       "      <td>0.785150</td>\n",
       "      <td>0.453054</td>\n",
       "      <td>0.574566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0250</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.936337</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.576409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.936949</td>\n",
       "      <td>0.805873</td>\n",
       "      <td>0.450319</td>\n",
       "      <td>0.577778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.938870</td>\n",
       "      <td>0.852575</td>\n",
       "      <td>0.437557</td>\n",
       "      <td>0.578313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0025</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.937473</td>\n",
       "      <td>0.884848</td>\n",
       "      <td>0.399271</td>\n",
       "      <td>0.550251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C  Non-Zero Coeffs  Accuracy  Precision    Recall  F1_score\n",
       "0  1.0000             30.0  0.935202   0.781300  0.449407  0.570602\n",
       "1  0.7500             30.0  0.935115   0.780063  0.449407  0.570272\n",
       "2  0.5000             29.0  0.935115   0.780063  0.449407  0.570272\n",
       "3  0.2500             29.0  0.935289   0.782540  0.449407  0.570932\n",
       "4  0.1000             27.0  0.935726   0.785150  0.453054  0.574566\n",
       "5  0.0500             26.0  0.935726   0.785150  0.453054  0.574566\n",
       "6  0.0250             24.0  0.936337   0.794872  0.452142  0.576409\n",
       "7  0.0100             21.0  0.936949   0.805873  0.450319  0.577778\n",
       "8  0.0050             18.0  0.938870   0.852575  0.437557  0.578313\n",
       "9  0.0025             11.0  0.937473   0.884848  0.399271  0.550251"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Listing all values I want to try as C value\n",
    "C=[1,0.75,0.50, 0.25, 0.1, 0.05, 0.025, 0.01, 0.005, 0.0025]\n",
    "#Intiate Metric of zeros with 6 columns and rows equal to len(C)\n",
    "l1_metrics = np.zeros((len(C),6))\n",
    "#Adding first column as value C\n",
    "l1_metrics[:,0]=C\n",
    "\n",
    "y_train_pd_r = y_train_pd.values.ravel()\n",
    "y_test_pd_r = y_test_pd.values.ravel()\n",
    "\n",
    "# Run a for loop over the range of C list length\n",
    "for index in range(0, len(C)):\n",
    "    # Initialize and fit Logistic Regression with the C candidate\n",
    "    logreg = LogisticRegression(penalty='l1', C=C[index], solver='liblinear')\n",
    "    logreg.fit(X_train_pd, y_train_pd_r)\n",
    "    # Predict on the testing data\n",
    "    y_pred_pd = logreg.predict(X_test_pd)\n",
    "    # Create non-zero count and all metrics columns\n",
    "    l1_metrics[index,1] = np.count_nonzero(logreg.coef_)\n",
    "    l1_metrics[index,2] = accuracy_score(y_test_pd_r, y_pred_pd)\n",
    "    l1_metrics[index,3] = precision_score(y_test_pd_r, y_pred_pd)\n",
    "    l1_metrics[index,4] = recall_score(y_test_pd_r, y_pred_pd)\n",
    "    l1_metrics[index,5] = f1_score(y_test_pd_r, y_pred_pd)\n",
    "\n",
    "col_names = ['C','Non-Zero Coeffs','Accuracy','Precision','Recall','F1_score']\n",
    "pd.DataFrame(l1_metrics, columns=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we would like to choose a model with reduced complexity that still maintains similar performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Feature      Coef\n",
      "0                  same_account -0.553069\n",
      "1                     same_bank -0.054793\n",
      "2                   day_of_week -0.041118\n",
      "3   transactions_same_hour_fata -0.026676\n",
      "4                   exists_same -0.009641\n",
      "5                   amount_paid  0.003366\n",
      "6      transactions_same_day_fa  0.009684\n",
      "7               amount_received  0.014734\n",
      "8                      weight_t  0.081677\n",
      "9                   ta_pagerank  0.084986\n",
      "10               before_forward  0.103211\n",
      "11                  fa_pagerank  0.133743\n",
      "12                 day_of_month  0.175623\n",
      "13               exists_similar  0.183032\n",
      "14               payment_format  0.234495\n",
      "15        from_account_inDegree  0.327314\n",
      "16               fan_out_degree  1.043126\n",
      "17                fan_in_degree  1.357693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonemalesardi/opt/anaconda3/envs/amd_sm2l/lib/python3.8/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#fiting Logistic Regression with C 0.1\n",
    "logreg = LogisticRegression(penalty='l1', C=0.005, solver='liblinear')\n",
    "logreg.fit(X_train_pd, y_train_pd)\n",
    "#We can call logreg.coef_ to get all coefficients\n",
    "#Lets use this to create data frame with Feature name & coefficient\n",
    "coef_df=pd.DataFrame(logreg.coef_,columns=X_train_pd.columns).T.reset_index()\n",
    "coef_df.columns=[\"Feature\",\"Coef\"]\n",
    "#Filter features with non zero coefficients and sort by coef value\n",
    "coef_df_final=coef_df[coef_df[\"Coef\"]!=0].sort_values(by=\"Coef\").reset_index(drop=True)\n",
    "#Lets print feature and its importance \n",
    "print(coef_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_features = ['id'] + coef_df_final['Feature'].tolist() + ['is_laundering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.select(reduced_features).write.parquet('src/datasets/reduced_train_C.005.parquet')\n",
    "test.select(reduced_features).write.parquet('src/datasets/reduced_test_C.005.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train = spark.read.parquet('src/datasets/reduced_train_C.005.parquet')\n",
    "reduced_test = spark.read.parquet('src/datasets/reduced_test_C.005.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "inputCols = reduced_train.drop('is_laundering','id').columns\n",
    " \n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "vectorized_train = assembler.transform(reduced_train)\n",
    "vectorized_test = assembler.transform(reduced_test)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"is_laundering\", featuresCol=\"features\")\n",
    "model = dt_classifier.fit(vectorized_train)\n",
    "\n",
    "predictions = model.transform(vectorized_test)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_laundering\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_sm2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
